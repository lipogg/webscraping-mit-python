
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>11.4. Weitere Tipps und Tricks &#8212; Webscraping für Geisteswissenschaften</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/11/subchapters/04_tipps_tricks';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="prev" title="11.3. Authentifizierung, Autorisierung und APIs" href="03_apis_tokens.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo.png" class="logo__image only-light" alt="Webscraping für Geisteswissenschaften - Home"/>
    <script>document.write(`<img src="../../../_static/logo.png" class="logo__image only-dark" alt="Webscraping für Geisteswissenschaften - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Webscraping mit Python für Geisteswissenschaften
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../01/01_intro.html">1. Einstieg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../01/subchapters/01_was_ist_webscraping.html">1.1. Was ist Web Scraping?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../01/subchapters/02_warum_webscraping_lernen.html">1.2. Motivation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../01/subchapters/03_semesterplan.html">1.3. Semesterplan</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../01/subchapters/04_lernziele.html">1.4. Lernziele</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../01/subchapters/05_organisation.html">1.5. Organisatorisches</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../02/02_intro.html">2. Python I</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../02/subchapters/01_jupyterlite.html">2.1. JupyterLite</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02/subchapters/01_style.html">2.2. Style Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02/subchapters/01_hilfe.html">2.3. Hilfe!!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02/subchapters/02_grundbegriffe.html">2.4. Grundbegriffe</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02/subchapters/03_datentypen.html">2.5. Einfache Datentypen, Strings und Operatoren</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02/subchapters/04_variablen.html">2.6. Variablen</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../03/03_intro.html">3. Python II</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../03/subchapters/01_datentypen.html">3.1. Zusammengesetzte Datentypen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../03/subchapters/02_kontrollstrukturen.html">3.2. Kontrollstrukturen</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../04/04_intro.html">4. Python III</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../04/subchapters/01_funktionen.html">4.1. Funktionen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04/subchapters/02_pakete.html">4.2. Module, Pakete, Bibliotheken</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../05/05_intro.html">5. Installation und Setup</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../05/subchapters/01_vorbereitung.html">5.1. Vorbereitung</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../05/subchapters/02_installation_setup.html">5.2. Installation und Setup</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../06/06_intro.html">6. Einstieg Webscraping</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../06/subchapters/01_rechtliches.html">6.1. Der rechtliche Rahmen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../06/subchapters/02_html.html">6.2. HTML</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../06/subchapters/03_css.html">6.3. CSS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../06/subchapters/04_http.html">6.4. Client, Server, HTTP</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../07/07_intro.html">7. Statische Webseiten</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../07/subchapters/01_einstieg_beautifulsoup.html">7.1. Einstieg BeautifulSoup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../07/subchapters/02_fortsetzung_beautifulsoup.html">7.2. Fortsetzung BeautifulSoup</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../08/08_intro.html">8. APIs (Exkurs)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../08/subchapters/01_apis.html">8.1. APIs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../08/subchapters/02_dracor_api.html">8.2. Beispiel DraCor-API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../08/subchapters/03_loc_api.html">8.3. Beispiel LOC-API</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../09/09_intro.html">9. Dynamische Webseiten</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../09/subchapters/01_statisch_vs_dynamisch.html">9.1. Statisch vs. Dynamisch?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../09/subchapters/02_einstieg_selenium.html">9.2. Einstieg Selenium</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../09/subchapters/03_fortsetzung_selenium.html">9.3. Fortsetzung Selenium</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../10/10_intro.html">10. Pandas (Exkurs)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../10/subchapters/01_pandas_bereinigen.html">10.1. Daten bereinigen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../10/subchapters/02_pandas_visualisieren.html">10.2. Daten visualisieren</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../11_intro.html">11. Tipps und Tricks</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_fetch_xhr.html">11.1. Selenium ist kein Hammer</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_sessions_cookies.html">11.2. Sessions und Cookies</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_apis_tokens.html">11.3. Authentifizierung, Autorisierung und APIs</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">11.4. Weitere Tipps und Tricks</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/lipogg/webscraping-mit-python" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/chapters/11/subchapters/04_tipps_tricks.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Weitere Tipps und Tricks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#user-agent-im-http-header-setzen">11.4.1. User Agent im HTTP-Header setzen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selenium-im-headless-mode">11.4.2. Selenium im Headless Mode</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#http-content-negotiation-und-linked-data">11.4.3. HTTP Content Negotiation und Linked Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regulare-ausdrucke">11.4.4. Reguläre Ausdrücke</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fehler-ausnahmen-und-ausnahmebehandlung">11.4.5. Fehler, Ausnahmen und Ausnahmebehandlung</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logging-statt-print-verwenden">11.4.6. Logging statt print() verwenden</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verschiedene-losungen-vergleichen">11.4.7. Verschiedene Lösungen vergleichen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-testen">11.4.8. Code testen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#robuster-scrapen-mit-requests">11.4.9. Robuster scrapen mit requests</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#versionsverwaltung-nutzen">11.4.10. Versionsverwaltung nutzen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strukturierung-von-python-projekten">11.4.11. Strukturierung von Python-Projekten</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webscraping-projekte-mit-mehreren-datenquellen">11.4.12. Webscraping-Projekte mit mehreren Datenquellen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sicher-scrapen">11.4.13. Sicher Scrapen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#effizienter-scrapen-mit-sitemaps">11.4.14. Effizienter Scrapen mit Sitemaps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wo-steht-nochmal">11.4.15. Wo steht nochmal…</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#don-t-do-this">11.4.16. Don’t do this…</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quellen">11.4.17. Quellen</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="weitere-tipps-und-tricks">
<h1><span class="section-number">11.4. </span>Weitere Tipps und Tricks<a class="headerlink" href="#weitere-tipps-und-tricks" title="Link to this heading">#</a></h1>
<section id="user-agent-im-http-header-setzen">
<h2><span class="section-number">11.4.1. </span>User Agent im HTTP-Header setzen<a class="headerlink" href="#user-agent-im-http-header-setzen" title="Link to this heading">#</a></h2>
<p>In manchen Fällen kann es notwendig sein, den Header der HTTP-Anfrage zu bearbeiten. Standardmäßig wird in einer HTTP-Anfrage über das requests-Paket als User Agent das requests-Paket angegeben:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Default-Header der HTTP-Anfrage</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&amp;phrasetext=book+review&amp;format=json&quot;</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">headers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;User-Agent&#39;: &#39;python-requests/2.32.3&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*/*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;}
</pre></div>
</div>
</div>
</div>
<p>Macht es einen Unterschied, ob im Header als User Agent python-requests/2.29.0 oder Mozilla/5.0 (bzw. ein anderer Browser) steht? Ja, denn mit dem Default-Header erkennt der Server, dass die Anfrage von einer Webscraping Bibliothek ausgeht.
Manche Websitebetreiber:innen behandeln solche Anfragen anders und schränken den Zugriff für solche Anfragen ein.
Allerdings darf der User Agent nicht immer geändert werden, denn manche Nutzungsbedingungen schließen so eine Art der “Vortäuschung” explizit aus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># User Agent im Request-Header ersetzen</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0&#39;</span><span class="p">}</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Wenn eine requests Session verwendet wird, muss der Header sogar nicht im Gesamten ersetzt werden, sondern es können einzelne Werte ganz einfach ausgetauscht werden. Laut requests-Dokumentationsseiten geht das wie folgt:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># s = requests.Session()</span>
<span class="c1"># s.headers.update({&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;})</span>
</pre></div>
</div>
</div>
</div>
<p>Wenn ihr Schnittstellen öffentlicher Institutionen nutzt, ist es <strong>oft empfehlenswert, als User Agent den Namen des Forschungsprojekts anzugeben</strong>. Manche Websitebetreiber:innen bitten sogar explizit darum, wie beispielsweise die Betreiber:innen der API der Gemeinsamen Normdatei (lobid-gnd):</p>
<blockquote>
<div><p>Wir bitten darum, bei der Nutzung von lobid eine aussagekräftige, wiederkehrende Zeichenkette als User Agent mitzusenden, damit wir bei der statistischen Auswertung unserer Infrastruktur Nutzungsweisen der API erkennen und unsere Dienstleistungen aus den gewonnenen Erkenntnissen verbessern können. Die statistische Erfassung der Nutzung dient außerdem der Begründung der Relevanz unserer Daten und Dienste gegenüber Geldgeber:innen und Entscheider:innen. In der Zeichenkette des User Agent kann sich die zugreifende Person, Institution oder ein Projekt zu erkennen geben, gegebenenfalls auch eine Kontaktmöglichkeit (E-Mail-Adresse) hinzufügen, eine anonyme beziehungsweise pseudonyme Kennung ist jedoch ebenso möglich. Eine solche Agent-Kennung sollte über die Dauer eines Projektes möglichst unverändert bleiben. (Quelle: <a class="reference external" href="https://lobid.org/usage-policy/">lobid-gnd</a>)</p>
</div></blockquote>
<p>Der Name des Forschungsprojekts kann einfach als String anstelle von “Mozilla/5.0” angegeben werden; dabei könnt ihr z.B. auch einen Link zu einem GitHub-Repository angeben, in dem sich der Code des Web Scrapers befindet, oder eine E-Mail-Adresse, unter der ihr kontaktiert werden wollt.</p>
</section>
<section id="selenium-im-headless-mode">
<h2><span class="section-number">11.4.2. </span>Selenium im Headless Mode<a class="headerlink" href="#selenium-im-headless-mode" title="Link to this heading">#</a></h2>
<p>Bisher haben wir Selenium immer so verwendet, dass sich beim Aufruf einer Seite der automatisierte Chrome Browser geöffnet hat. Wenn ihr gerade dabei seid, den Code zu schreiben, die Web Scraping-Strategie zu planen und die extrahierten Daten zu überprüfen, dann empfiehlt es sich, den Browser in diesem “sichtbaren” Modus zu automatisieren. Aber wenn das Skript dann fertig ist und ihr nicht mehr im Browserfenster nachverfolgen müsst, ob alle Aktionen so ausgeführt werden wie gewünscht, könnt ihr den Browser auch im sogenannten “headless mode” automatisieren, bei dem sich das Browserfenster nicht öffnet:</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">selenium</span><span class="w"> </span><span class="kn">import</span> <span class="n">webdriver</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">selenium.webdriver.chrome.options</span><span class="w"> </span><span class="kn">import</span> <span class="n">Options</span>

<span class="n">options</span> <span class="o">=</span> <span class="n">Options</span><span class="p">()</span>
<span class="n">options</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--headless&quot;</span><span class="p">)</span>
<span class="n">driver</span> <span class="o">=</span> <span class="n">webdriver</span><span class="o">.</span><span class="n">Chrome</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Der Webdriver kann mit weiteren Optionen konfiguriert werden. Eine Übersicht über alle Optionen findet ihr in den <a class="reference external" href="https://www.selenium.dev/documentation/webdriver/drivers/options/">Selenium-Dokumentationsseiten</a>.</p>
</section>
<section id="http-content-negotiation-und-linked-data">
<h2><span class="section-number">11.4.3. </span>HTTP Content Negotiation und Linked Data<a class="headerlink" href="#http-content-negotiation-und-linked-data" title="Link to this heading">#</a></h2>
<p>Im Zusammenhang mit REST APIs haben wir gesehen, dass verschiedene Dateiformate meist durch Angabe eines Parameters (z.B.: <code class="docutils literal notranslate"><span class="pre">api/items/3492?format=json</span></code>) oder durch Angabe der Dateiendung (z.B. <code class="docutils literal notranslate"><span class="pre">api/items/3492.json</span></code>) angefragt werden können. Allerdings sieht die HTTP Spezifikation, also die Sammlung von Dokumenten, in denen das HTTP-Protokoll beschrieben wird, einen eigenen Mechanismus für die Auswahl verschiedener Dateiformate vor, der sich “Content Negotiation” nennt. Dabei wird das Format einer Ressource direkt über den Accept-Parameter im HTTP-Header “ausgewählt”, oder, etwas genauer ausgedrückt, zwischen Client und Server “ausgehandelt” (engl. “negotiation”). Dieser Mechanismus wird insbesondere im Zusammenhang mit sogenannten Linked Data Datensätzen verwendet. Linked Data ist grob gesagt ein Ansatz, strukturierte Daten im Web bereitzustellen. Solche Daten sind meist nach einer ganz bestimmten Syntax und Logik strukturiert, die wir im Rahmen dieses Seminars nicht vertiefen können, und sie werden als Turtle, RDF-XML oder JSON-LD Dateien bereitgestellt. Viele Organisationen, kulturelle Einrichtungen, Museen, Bibliotheken und Archive stellen neben klassischen REST APIs wie wir sie kennengelernt haben auch Schnittstellen zum Abrufen von Linked Data bereit, bei deren Nutzung das Dateiformat im Accept-Header mit <code class="docutils literal notranslate"><span class="pre">Accept:</span> <span class="pre">text/turtle</span></code>, <code class="docutils literal notranslate"><span class="pre">Accept:</span> <span class="pre">application/rdf+xml</span></code> oder <code class="docutils literal notranslate"><span class="pre">Accept:</span> <span class="pre">application/ld+json</span></code> angefordert wird (statt <code class="docutils literal notranslate"><span class="pre">Accept:</span> <span class="pre">text/html</span></code> wie beim Web Scrapen oder <code class="docutils literal notranslate"><span class="pre">Accept:</span> <span class="pre">application/json</span></code> wie bei der Abfrage von Daten über REST APIs).</p>
<p>Um Linked Data sinnvoll nutzen zu können, braucht ihr allerdings etwas Hintergrundwissen. Als Einstieg in das Thema Linked Data (und die damit zusammenhängenden Themengebiete Semantic Web und Knowledge Graphs) empfehle ich die hervorragenden Kurse des Hasso-Plattner-Instituts zu den Themen <a class="reference external" href="https://www.youtube.com/watch?v=Q5DrZV5wWzo&amp;amp;list=PLoOmvuyo5UAeihlKcWpzVzB51rr014TwD">Semantic Web Technologies</a> und <a class="reference external" href="https://open.hpi.de/courses/knowledgegraphs2023/">Knowledge Graphs</a>.</p>
</section>
<section id="regulare-ausdrucke">
<h2><span class="section-number">11.4.4. </span>Reguläre Ausdrücke<a class="headerlink" href="#regulare-ausdrucke" title="Link to this heading">#</a></h2>
<p>Reguläre Ausdrücke (engl. Regular Expression, kurz: RegEx, RegExp) sind verallgemeinerte Suchmuster (patterns) für Zeichenketten. Mithilfe von regulären Ausdrücken können syntaktische Konstrukte so beschrieben werden, dass sie ein Computer versteht. Ein syntaktisches Konstrukt ist zum Beispiel eine Zahl zwischen 1900 und 2000, eine Telefonnummer, eine Adresse, eine URL oder auch ein bestimmtes Wort in verschiedenen Flexionsformen. Mithilfe von regulären Ausdrücken können also Texte nach bestimmten Mustern durchsucht werden, und die gefundenen Konstrukte können anschließend z.B. entfernt oder bearbeitet werden. Die meisten Programmiersprachen, darunter auch Python, stellen Funktionen bereit, welche die Verwendung von regulären Ausdrücken erlauben.</p>
<p>Bei der Bereinigung der extrahierten Daten sind reguläre Ausdrücke oft sehr nützlich. Deswegen lohnt es sich, sich mit dieser speziellen Sprache etwas zu beschäftigen und die Syntax zumindest in den Grundzügen zu verstehen. Als Einführung empfehle ich das <a class="reference external" href="https://docs.python.org/3/howto/regex.html">Regular Expressions HOWTO</a> in den Python-Dokumentationsseiten, und für einen schnellen Überblick über die Syntax empfehle ich <a class="reference external" href="https://www.dataquest.io/cheat-sheet/regular-expressions-cheat-sheet/">dieses Cheatsheet</a>, und für fortgeschrittene Themen empfehle ich <a class="reference external" href="https://www.regular-expressions.info/tutorial.html">regular-expressions.info</a>. Regex-Suchmuster könnt ihr auch online testen, zum Beispiel auf der Seite <a class="reference external" href="https://regex101.com/">https://regex101.com/</a>.</p>
<p>Wir betrachten im Folgenden zwei typische Anwendungsfälle für reguläre Ausdrücke in Web Scraping Projekten.</p>
<p><strong>Anwendungsbeispiel 1: Suche nach bestimmten Mustern in den Daten</strong></p>
<p>In einer der letzten Übungsaufgaben solltet ihr mithilfe der Methode <code class="docutils literal notranslate"><span class="pre">.str.contains()</span></code> überprüfen, ob die von der Seite Pinterest extrahierten Kommentare die Zeichenkette “cute” enthalten. Die Methode akzeptiert laut den <a class="reference external" href="https://pandas.pydata.org/docs/reference/api/pandas.Series.str.contains.html">Pandas Dokumentationsseiten</a> als Argument eine “character sequence or regular expression”. Anstatt nur nach dem Wort “cute” zu suchen, könntet ihr einen regulären Ausdruck formulieren, der nach verschiedenen Varianten des Wortes und nach Synonymen sucht:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># comments_df[&quot;comment&quot;].str.contains(&quot;(cuti?e)|(sweet(ie)?)&quot;)</span>
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Verständnisfrage</p>
<ul class="simple">
<li><p>Welche Wörter werden mit diesem Suchmuster gefunden?</p></li>
<li><p>Wird auch die Zeichenkette “sweeti” gefunden?</p></li>
</ul>
<p>Probiert es aus: Fügt das Suchmuster und einen Beispieltext auf <a class="reference external" href="https://regex101.com/">https://regex101.com/</a> ein.</p>
</div>
<p><strong>Anwendungsbeispiel 2: Dateinamen vor dem Speichern bearbeiten</strong></p>
<p>Bisher haben wir Metadaten immer genauso, wie sie sind, als Dateinamen verwendet. Das ist aber nicht immer möglich oder gewünscht. Dateinamen können mithilfe von regulären Ausdrücken und der Funktion sub() aus dem Modul re aus der Python-Standardbibliothek bearbeitet werden. Die Funktion sub() tauscht alle Sequenzen in einem String, die einem gesuchten Muster entsprechen, gegen einen neuen String aus. Die Funktion nimmt einen regulären Ausdruck, einen String, gegen den die gefundenen Sequenzen ausgetauscht werden sollen, und einen String, in dem gesucht werden soll, als Argumente an. Ein Beispiel:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">re</span>

<span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Arizona sun. [volume]_19550121.txt&quot;</span>
<span class="n">title</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">. </span><span class="se">\\</span><span class="s2">[volume</span><span class="se">\\</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="c1"># Zusatz [volume] entfernen</span>
<span class="n">title</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="c1"># Leerzeichen durch Unterstriche ersetzen</span>
<span class="n">title</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Arizona_sun_19550121.txt&#39;
</pre></div>
</div>
</div>
</div>
<p>Für weitere Funktionen zur Arbeit mit regulären Ausdrücken schaut in die <a class="reference external" href="https://docs.python.org/3/library/re.html">Dokumentationsseiten zum Modul re</a>.</p>
</section>
<section id="fehler-ausnahmen-und-ausnahmebehandlung">
<h2><span class="section-number">11.4.5. </span>Fehler, Ausnahmen und Ausnahmebehandlung<a class="headerlink" href="#fehler-ausnahmen-und-ausnahmebehandlung" title="Link to this heading">#</a></h2>
<p>In Python gibt es verschiedene Arten von Fehlern: zum einen Syntaxfehler (syntax errors), die verhindern, dass Code überhaupt erst ausgeführt werden kann, und zum anderen Ausnahmen (exceptions), also Fehler, die dazu führen, dass die Ausführung des Codes abgebrochen und eine Fehlermeldung angezeigt wird. Bisher haben wir immer umgangssprachlich gesagt, dass eine Fehlermeldung “angezeigt” wird. Ganz korrekt würde man aber eigentlich sagen, dass eine Ausnahme “geworfen” wird. Eine Liste der Ausnahmen, welche die Python Standard Library definiert und die beim Ausführen von Python-Code geworfen werden können, findet sich hier: <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#bltin-exceptions">https://docs.python.org/3/library/exceptions.html#bltin-exceptions</a>.</p>
<p>Grundsätzlich sollte euer Code vorbeugend mit möglichen Fehlerquellen umgehen: Netzwerkanfragen können immer auch scheitern, und diese Fälle sollten immer registriert und aufgefangen werden. Und auch die Daten, die ihr extrahiert, können unterschiedlich strukturiert sein, und unterschiedliche Weiterverarbeitungsschritte erfordern. Bei der Wahl des Dateinamens solltet ihr bedenken, dass Dateinamen zum Beispiel nicht unendlich lang sein dürfen, oder dass Schrägstriche in den Dateinamen beim Schreiben der Dateien fälschlich als Dateipfade interpretiert werden können (das haben wir im Zusammenhang mit der LOC API bemerkt). In beiden Fällen würde jeweils eine Ausnahme geworfen werden und die Ausführung des Codes würde abbrechen. Ihr solltet euren Code also immer vorausschauend schreiben. Im Beispiel mit den Dateinamen könnten vorausschauend Metadaten für die Dateinamen ausgewählt werden, welche immer dieselbe Form haben und somit immer gültig sind, oder indem die Dateinamen vor dem Schreiben der Dateien vereinheitlicht werden. Aber nicht immer kann im Voraus abgeschätzt werden, welche Art von ungültigen Werten auftreten können. Für diesen Fall gibt es in Python spezielle Anweisungen, die erlauben, bestimmte Ausnahmen gezielt abzufangen und so ein vorzeitiges Beenden des Programms zu verhindern: sogenannte try/except-Anweisungen.</p>
<p>Try/except-Anweisungen haben die allgemeine Form:</p>
<figure class="align-default" id="id245">
<a class="bg-transparent reference internal image-reference" href="../../../_images/try_except.png"><img alt="Try/Except-Anweisungen" class="bg-transparent" src="../../../_images/try_except.png" style="width: 75%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 11.13 </span><span class="caption-text">Try/Except-Anweisungen in der allgemeinen Form</span><a class="headerlink" href="#id245" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Der Code im except-Zweig wird dabei nur dann ausgeführt, wenn beim Ausführen des Codes im try-Zweig eine Ausnahme geworfen wird. Diese einfache try/except-Anweisung kann um weitere Zweige ergänzt werden. Eine komplexere try-except-Struktur könnte ungefähr so aussehen:</p>
<figure class="align-default" id="id246">
<a class="bg-transparent reference internal image-reference" href="../../../_images/try_except_explained.png"><img alt="Try/Except-Anweisungen" class="bg-transparent" src="../../../_images/try_except_explained.png" style="width: 75%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 11.14 </span><span class="caption-text">Schaubild: try/except-Anweisung mit mehreren Zweigen. Adaptiert von <a class="reference external" href="https://openbook.rheinwerk-verlag.de/python/22_001.html">Ernesti und Kaiser (2020)</a>.</span><a class="headerlink" href="#id246" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Der folgende Beispielcode fängt Ausnahmen, die durch das Paket requests geworfen werden, ab, indem es die Ausnahme loggt. Beim Auftreten einer Ausnahme wird die Ausführung jedoch nicht abgebrochen, sondern es wird None zurückgegeben und danach fährt der Code mit der nächsten URL fort.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># Funktion, die getestet werden soll</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_page</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
        <span class="c1"># Wenn der Statuscode im 400er oder 500er Bereich liegt, wird eine Requests Exception (HTTPError) geworfen.</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span>
    <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exception of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> occurred while fetching </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">exc_info</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">None</span>

<span class="n">req_url</span> <span class="o">=</span> <span class="s2">&quot;https://example.com&quot;</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">get_page</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">req_url</span><span class="si">}</span><span class="s2">/page=</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">response</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Dieser Fall muss hier behandelt werden</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Ausnahmen sollten immer so spezifisch wie möglich abgefangen werden. Prinzipiell ist es möglich, mit <code class="docutils literal notranslate"><span class="pre">except</span> <span class="pre">Exception</span></code> alle möglichen Ausnahmen gleichzeitig abzufangen, aber das ist laut Ernesti und Kaiser (sowie den allermeisten seriösen Quellen zufolge) fast nie sinnvoll und kein guter Stil.</p>
</div>
<p>Neben dem Abfangen von Ausnahmen gibt es in Python auch die Möglichkeit, eigene Ausnahmen zu werfen oder Ausnahmen an den aufrufenden Code weiterzugeben. Dazu wird das Schlüsselwort <code class="docutils literal notranslate"><span class="pre">raise</span></code> verwendet:</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_page</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
        <span class="c1"># Wirft HTTPError bei Statuscodes 4xx / 5xx</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span>

    <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exception of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> occurred while fetching </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">exc_info</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># Ausnahme an aufrufenden Code weiterreichen</span>
        <span class="k">raise</span>

<span class="n">req_url</span> <span class="o">=</span> <span class="s2">&quot;https://example.com&quot;</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">get_page</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">req_url</span><span class="si">}</span><span class="s2">/page=</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Verständnisfragen:</p>
<ul class="simple">
<li><p>Überlegt euch ein Szenario, in dem es sinnvoll sein könnte, beim Scheitern einer Anfrage None zurückzugeben und die nächste Seite zu scrapen.</p></li>
<li><p>Überlegt euch ein Szenario, in dem dieses Vorgehen problematisch ist, und es besser ist, wenn die Ausführung des Codes komplett abbricht.</p></li>
<li><p>Was sollte in den beiden Szenarien mit den URLs der Seiten geschehen, die nicht erfolgreich gescraped werden konnten?</p></li>
</ul>
</div>
<p>Weitere Beispiele und Erläuterungen zum Thema Ausnahmebehandlung findet ihr im erwähnten <a class="reference external" href="https://openbook.rheinwerk-verlag.de/python/22_001.html">Handbuch von Ernesti und Kaiser</a>, in <a class="reference external" href="https://realpython.com/python-exceptions/">diesem Beitrag von Said van de Klundert</a>, und natürlich in den offiziellen <a class="reference external" href="https://docs.python.org/3/tutorial/errors.html">Python-Dokumentationsseiten</a>.</p>
</section>
<section id="logging-statt-print-verwenden">
<h2><span class="section-number">11.4.6. </span>Logging statt print() verwenden<a class="headerlink" href="#logging-statt-print-verwenden" title="Link to this heading">#</a></h2>
<p>Insbesondere im Zusammenhang mit Fehler- und Ausnahmebehandlung und bei Skripten mit längeren Ausführungszeiten ist es häufig unpraktisch, wenn Warnungen nur mit print() auf dem Bildschirm ausgegeben werden. Praktischer wäre es, wenn Meldungen abgestuft nach Dringlichkeit und Art und mit einer Zeitangabe versehen in einer Logdatei gespeichert würden. Um dem eigenen Webscraping-Projekt einen derartigen Logging-Mechanismus hinzuzufügen, kann das Modul <code class="docutils literal notranslate"><span class="pre">logging</span></code> aus der Python-Standardbibliothek verwendet werden. Die Dringlichkeitsstufen im logging-Modul sind DEBUG, INFO, WARNING, ERROR und CRITICAL und dienen dazu, verschiedene Arten von Ereignissen zu beschreiben, die beim Ausführen des Codes auftreten.</p>
<p>In den <a class="reference external" href="https://docs.python.org/3/howto/logging.html#logging-basic-tutorial">Python-Dokumentationsseiten</a> findet sich dieses Beispiel, das illustriert, wie das logging-Modul ohne Setup mit den Defaulteinstellungen verwendet werden kann:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Watch out!&#39;</span><span class="p">)</span>  <span class="c1"># will print a message to the console</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;I told you so&#39;</span><span class="p">)</span>  <span class="c1"># will not print anything</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:root:Watch out!
</pre></div>
</div>
</div>
</div>
<p>Das Problem dabei ist, dass nur Nachrichten mit dem Level WARNING ausgegeben werden, und dass diese direkt im Notebook ausgegeben werden und nicht in eine Logdatei geschrieben werden. Um eine Logdatei anzulegen und einzustellen, welche Dringlichkeitsstufe dort registriert werden soll, kann dieser Code verwendet werden, der ebenfalls in leicht adaptierter Form den Dokumentationsseiten entnommen ist:</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;example.log&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1"> </span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;This message should go to the log file&#39;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;So should this&#39;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;And this, too&#39;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s1">&#39;And non-ASCII stuff, too, like Øresund and Malmö&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Weitere Informationen zum Logging findet ihr hier:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/logging.html">https://docs.python.org/3/library/logging.html</a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/howto/logging.html#logging-basic-tutorial">https://docs.python.org/3/howto/logging.html#logging-basic-tutorial</a></p></li>
</ul>
</section>
<section id="verschiedene-losungen-vergleichen">
<h2><span class="section-number">11.4.7. </span>Verschiedene Lösungen vergleichen<a class="headerlink" href="#verschiedene-losungen-vergleichen" title="Link to this heading">#</a></h2>
<p>In der Sitzung “Fortsetzung BeautifulSoup” haben wir versucht, verschiedene Lösungen auf eine recht naive Art zu vergleichen, indem wir einfach die Zeit, die beim einmaligen Ausführen einer Codezelle verstrichen ist, gemessen haben. Die Ausführungszeit kann aber beim wiederholten Ausführen desselben Codeblocks schwanken. Um eine etwas genauere Messung zu erhalten, kann anstelle von <code class="docutils literal notranslate"><span class="pre">%%time</span></code> <code class="docutils literal notranslate"><span class="pre">%timeit</span></code> verwendet werden, um die durchschnittliche Ausführungszeit einer Funktion bei mehreren wiederholten Aufrufen zu messen. Als Beispiel messen wir die Ausführungszeit der is_even()-Funktion aus <a class="reference external" href="https://lipogg.github.io/webscraping-mit-python/chapters/04/subchapters/01_funktionen.html">Abchnitt 4.1</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">is_even</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Input: i, a positive int</span>
<span class="sd">    Returns True if i is even, otherwise False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">i</span><span class="o">%</span><span class="k">2</span> == 0

<span class="o">%</span><span class="k">timeit</span> is_even(10)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>74.7 ns ± 0.0569 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)
</pre></div>
</div>
</div>
</div>
<p>Angenommen, wir haben eine weitere Lösung für dasselbe Problem gefunden: Anstatt den Modulo-Operator zu verwenden, um zu bestimmen, ob die angegebene Zahl durch 2 teilbar ist, reduzieren wir die Zahl in einer Schleife immer weiter um 2, bis 0 erreicht ist (oder nicht). Dann können wir die durchschnittliche Ausführungszeit dieser Lösung mit der ersten vergleichen:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">is_even_v2</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Input: i, a positive int</span>
<span class="sd">    Returns True if i is even, otherwise False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">-=</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span>

<span class="o">%</span><span class="k">timeit</span> is_even_v2(10)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>244 ns ± 1.96 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
</pre></div>
</div>
</div>
</div>
<p>Der Vergleich der durchschnittlichen Ausführungszeit kann einen ersten Anhaltspunkt geben, welche der zwei Lösungen effizienter im Hinblick auf die Ausführungszeit ist.</p>
<p>Gerade beim Web Scrapen hängt die Ausführungszeit aber auch von Faktoren ab, die wir gar nicht kontrollieren können und die sich von Ausführung zu Ausführung unterscheiden können, zum Beispiel der Zeit, die der Server braucht, um eine Antwort zu senden. Deswegen reicht der Vergleich der durchschnittlichen Ausführungszeit oft alleine nicht aus. Die gemessene Laufzeit lässt beispielsweise keine Rückschlüsse darüber zu, wie sich die Laufzeit entwickelt, wenn nicht nur 10 oder 20 sondern 100, 1000 oder 10000 Anfragen gestellt und die extrahierten Daten verarbeitet werden sollen. Mit einer theoretischen Analyse der Laufzeit, also der Bestimmung der sogenannten “<strong>Laufzeitkomplexität</strong>” des Codes, kann untersucht werden, wie sich die Laufzeit bei einer immer größer werdenden Anzahl von Eingabe-URLs unabhängig von äußeren Einflüssen wie dem Rate Limit oder der Antwortzeit des Servers entwickelt. Einen leicht verständlichen Einstieg in das Thema findet ihr <a class="reference external" href="https://www.youtube.com/watch?v=p65AHm9MX80">hier</a>. Einen fundierten Einstieg findet ihr <a class="reference external" href="https://youtu.be/7vUGHpbQABk?si=8FsyfJkPNQTjhZv-">hier</a>. Sehr, sehr kurz gefasst: Die theoretische Laufzeit wird anhand der Anzahl der grundlegenden Operationen in dem Code verglichen. Bei jedem Funktionsaufruf der Funktion <code class="docutils literal notranslate"><span class="pre">is_even()</span></code> finden beispielsweise nur zwei Operationen statt: Der Ausdruck i % 2 == 0 berechnet den Rest der Division von i durch 2 und prüft das Ergebnis auf Wertgleichheit mit dem Wert 0. Es findet also die gleiche Anzahl an Operationen statt, egal welche Zahl für i eingesetzt wird. Eine solche Laufzeit wird auch <strong>konstante Laufzeit</strong> genannt. Beim Aufruf der Funktion <code class="docutils literal notranslate"><span class="pre">is_even_v2()</span></code> finden jedoch mehrere Operationen statt, wobei die Anzahl der Operationen steigt, wenn i größer ist, weil die while-Schleife für größere Zahlen mehr Durchläufe braucht, bis i &lt; 1 ist. Eine solche Laufzeit wird <strong>lineare Laufzeit</strong> genannt, weil die Anzahl der Operationen proportional mit der Eingabegröße wächst. Die theoretische Laufzeit der Funktion <code class="docutils literal notranslate"><span class="pre">is_even()</span></code> deswegen deutlich geringer als die der Funktion <code class="docutils literal notranslate"><span class="pre">is_even_v2()</span></code>. In der formalen Syntax zur Beschreibung von Laufzeitkomlexität (der sogenannten O-Notation) wird die konstante Laufzeit der Funktion <code class="docutils literal notranslate"><span class="pre">is_even()</span></code> als O(1) notiert und die lineare Laufzeit der Funktion <code class="docutils literal notranslate"><span class="pre">is_even_v2()</span></code> als O(n).</p>
</section>
<section id="code-testen">
<h2><span class="section-number">11.4.8. </span>Code testen<a class="headerlink" href="#code-testen" title="Link to this heading">#</a></h2>
<p>Im Laufe des Semesters haben wir immer die Ausführung des Codes mit print() überwacht und den Output am Ende manuell überprüft. Dabei haben wir z.B. überprüft, ob der Code die erwartete Anzahl von Ergebnissen geliefert hat, welche Datentypen Funktionen zurückgeben, wenn beim Scrapen keine HTML-Elemente gefunden werden und was in diesem Fall passiert, und wir haben stichprobenartig untersucht, ob die Ergebnisse auch vollständig erscheinen. Diese Überprüfungsschritte werden bei größeren Softwareprojekten in Form von Tests formalisiert. Wenn ihr Code für eine Publikation oder eine Abschlussarbeit schreibt, solltet ihr auf jeden Fall Tests hinzufügen und euch eigenständig etwas in das Thema einlesen.</p>
<p>Das Ziel von Softwaretests ist es, zu überprüfen, ob sich der Code sowohl im Normalfall als auch im Fehlerfall wie erwartet verhält. Um das zu überprüfen, kann versucht werden, bewusst ein falsches, ungewolltes Verhalten des Programms herbeizuführen, also zum Beispiel einen unkontrollierten Abbruch mit einer Fehlermeldung, oder unvollständig extrahierte oder andere als die erwarteten Daten. Wenn solch ein Verhalten auftritt, dann liegt sehr wahrscheinlich ein Defekt (umgangssprachlich “Fehler”, siehe dazu mein Kommentar im <a class="reference external" href="https://lipogg.github.io/webscraping-mit-python/chapters/04/subchapters/01_funktionen.html#was-sind-methoden">Abschnitt 4.1.9</a>) im Code vor, der dann ausgebessert werden muss. Die Tests sollen dabei für eine möglichst breite Abdeckung sorgen, es sollen also alle wichtigen Fälle und Pfade, die bei der Ausführung durchlaufen werden können, einmal getestet werden (also z.B. jede Verzweigung bei einer if-else-Verzweigung, die wichtigsten HTTP-Statuscodes, Grenzwerte und Wertebereiche, die eine Variable annehmen kann).</p>
<p>Web Scraper zu testen ist aber nicht ganz so einfach, weil in der Regel in einem ersten Schritt mithilfe sogenannter “Unit Tests” alle Teile des Codes separat getestet werden und es dabei hinderlich ist, wenn beim Testen tatsächlich Netzwerkanfragen gestellt werden. Denn wenn aus einem Test heraus Anfragen gestellt werden, dann würden wir ja überprüfen, ob das requests-Paket sich wie erwartet verhält, und nicht z.B. die Funktion, die wir selbst geschrieben haben, aus der heraus die Anfragen gestellt werden. Außerdem würden wir den Server mit unnötigen Testanfragen zusätzlich belasten. Deswegen müssen HTTP-Anfragen in Unit Tests oft “gemockt” werden, das heißt, es muss ein Ersatzobjekt definiert werden, das den Teil des Codes, der die Anfrage stellt, in dem Test ersetzt. Solche Tests können mithilfe der beiden Pakete pytest und pytest-mock geschrieben werden. Beide Pakete müssen dazu zunächst installiert werden, aber nur pytest muss importiert werden. Tests werden gewöhnlich nicht in Jupyter Notebooks geschrieben, sondern in Dateien mit der Dateiendung .py. Eine auf das Notwendigste reduzierte Version eines solchen Tests könnte so aussehen:</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Datei scraper.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># Funktion, die getestet werden soll</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_page</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
        <span class="c1"># Wenn der Statuscode im 400er oder 500er Bereich liegt, wird eine Requests Exception (HTTPError) geworfen.</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span>
    <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exception of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> occurred while fetching </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">exc_info</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">None</span>

<span class="c1"># Datei test_scraper.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scraper</span> <span class="c1"># Scraper-Modul (Datei scraper.py) importieren</span>

<span class="c1"># Erste Testfunktion, die den Normalfall testet, bei dem die Anfrage erfolgreich ist, keine Ausnahme geworfen wird und die Funktion den Inhalt der Antwort als HTML-String zurückgibt</span>
<span class="c1"># Der Name der Datei, in der die Funktion get_page() definiert wird, steht dann vor get_page in der Testfunktion, also z.B. name_der_datei.get_page; hier also scraper.get_page</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_get_page</span><span class="p">(</span><span class="n">mocker</span><span class="p">):</span>
    <span class="n">html_content</span> <span class="o">=</span> <span class="s2">&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Das ist ein Test&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&quot;</span>
    <span class="n">mock_response</span> <span class="o">=</span> <span class="n">mocker</span><span class="o">.</span><span class="n">Mock</span><span class="p">()</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">text</span> <span class="o">=</span> <span class="n">html_content</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">mock_get</span> <span class="o">=</span> <span class="n">mocker</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="s2">&quot;scraper.requests.get&quot;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">scraper</span><span class="o">.</span><span class="n">get_page</span><span class="p">(</span><span class="s2">&quot;https://www.example.com&quot;</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="c1"># Überprüfen, ob die Funktion die HTML-Datei als String zurückgibt, wenn die Anfrage erfolgreich ist</span>
    <span class="k">assert</span> <span class="n">result</span> <span class="o">==</span> <span class="n">html_content</span>
    <span class="c1"># Überprüfen, ob die Funktion wirklich requests.get aufgerufen hat und eine Anfrage an &quot;https://www.example.com&quot; gestellt hat</span>
    <span class="n">mock_get</span><span class="o">.</span><span class="n">assert_called_once_with</span><span class="p">(</span><span class="s2">&quot;https://www.example.com&quot;</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="c1"># Überprüfen, ob raise_for_status() aufgerufen und der HTTP Statuscode der Antwort überprüft wurde</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="o">.</span><span class="n">assert_called_once</span><span class="p">()</span>

<span class="c1"># Zweite Testfunktion, die den Fehlerfall testet, bei dem requests.get() selbst eine RequestException auslöst, z.B. bei Netzwerkfehlern, Timeouts oder ungültiger URL wie hier im Beispiel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_get_page_exception</span><span class="p">(</span><span class="n">mocker</span><span class="p">):</span>
    <span class="n">mock_get</span> <span class="o">=</span> <span class="n">mocker</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="s2">&quot;scraper.requests.get&quot;</span><span class="p">,</span> <span class="n">side_effect</span><span class="o">=</span><span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RequestException</span><span class="p">())</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">scraper</span><span class="o">.</span><span class="n">get_page</span><span class="p">(</span><span class="s2">&quot;some-invalid-page&quot;</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="c1"># Überprüfen, ob die Funktion None zurückgibt, wenn eine Exception auftritt</span>
    <span class="k">assert</span> <span class="n">result</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="c1"># Überprüfen, ob die Funktion versucht hat, eine Anfrage an &quot;some-invalid-page&quot; zu stellen</span>
    <span class="n">mock_get</span><span class="o">.</span><span class="n">assert_called_once_with</span><span class="p">(</span><span class="s2">&quot;some-invalid-page&quot;</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Dritte Testfunktion, die den Fehlerfall testet, bei dem die Anfrage zwar eine Antwort liefert, aber raise_for_status() aufgrund eines HTTP-Statuscodes im 4xx- oder 5xx-Bereich eine HTTPError-Exception auslöst</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_get_page_http_error</span><span class="p">(</span><span class="n">mocker</span><span class="p">):</span>
    <span class="n">mock_response</span> <span class="o">=</span> <span class="n">mocker</span><span class="o">.</span><span class="n">Mock</span><span class="p">()</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="o">.</span><span class="n">side_effect</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">HTTPError</span><span class="p">()</span>
    <span class="n">mocker</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span><span class="s2">&quot;scraper.requests.get&quot;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">scraper</span><span class="o">.</span><span class="n">get_page</span><span class="p">(</span><span class="s2">&quot;https://example.com&quot;</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="c1"># Überprüfen, ob die Funktion None zurückgebt, wenn durch raise_for_status() ein HTTP-Fehler geworfen wird</span>
    <span class="k">assert</span> <span class="n">result</span> <span class="ow">is</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
<p>Die Tests werden normalerweise über die Kommandozeile mit dem Befehl <code class="docutils literal notranslate"><span class="pre">pytest</span> <span class="pre">test_scraper.py</span></code> ausgeführt (siehe dazu Abschnitt 11.4.10. “Strukturierung von Python-Projekten”). <code class="docutils literal notranslate"><span class="pre">test_scraper.py</span></code> ist dabei der Name der Datei, in der die Testfunktionen stehen. Wenn die Tests erfolgreich sind, steht in der Ausgabe neben dem Namen der jeweiligen Testfunktion PASSED. Nach den Unit Tests folgen meist weitere Tests, z.B. Integrationstests und Lasttests.</p>
<p>Einen praxisorientierten Einstieg in das Thema Softwaretests im Rahmen von Forschungsprojekten bietet die Seite <a class="reference external" href="https://carpentries-incubator.github.io/better-research-software/06-code-correctness.html">https://carpentries-incubator.github.io/better-research-software/06-code-correctness.html</a>. Die Pytest-Dokumentationsseiten finden sich unter <a class="reference external" href="https://docs.pytest.org/en/stable/index.html">https://docs.pytest.org/en/stable/index.html</a> und die Pytest-mock-Dokumentationsseiten unter <a class="reference external" href="https://pytest-mock.readthedocs.io/en/latest/">https://pytest-mock.readthedocs.io/en/latest/</a>. <a class="reference external" href="https://www.youtube.com/watch?v=XtuK6rKQ-dU">Dieses Video</a> illustriert, wie ein ähnlicher Test für Anfragen, die eine JSON-Antwort liefern, aussehen könnte.</p>
</section>
<section id="robuster-scrapen-mit-requests">
<h2><span class="section-number">11.4.9. </span>Robuster scrapen mit requests<a class="headerlink" href="#robuster-scrapen-mit-requests" title="Link to this heading">#</a></h2>
<p>Bisher haben wir mit requests immer sehr simple Anfragen gestellt. In der Praxis werden aber besonders bei größeren Projekten einige zusätzliche Einstellungen notwendig. Dazu gehören unter anderem:</p>
<ul class="simple">
<li><p>Retries: Wie oft sollen bei einer gescheiterten Anfrage erneute Anfragen gestellt werden?</p></li>
<li><p>Timeouts: Wie lange soll auf eine Antwort des Servers gewartet werden?</p></li>
<li><p>Ausnahmebehandlung: Was soll bei ungültigen Statuscodes, Netzwerkfehlern oder Timeouts gemacht werden? Wie soll mit Weiterleitungen (Statuscodes im 300er Bereich) umgegangen werden?</p></li>
</ul>
<p>Den Statuscode haben wir bisher mit if-Anweisungen der Art <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">response.status_code</span> <span class="pre">==</span> <span class="pre">200</span></code> überprüft. Dafür bietet requests eine wesentlich kürzere und robustere Schreibweise: <code class="docutils literal notranslate"><span class="pre">response.raise_for_status()</span></code>.</p>
<p>Wir haben bereits im <a class="reference external" href="https://lipogg.github.io/webscraping-mit-python/chapters/08/subchapters/03_loc_api.html#rate-limits-berucksichtigen-und-die-abfragerate-steuern">Abschnitt 8.3</a> im Zusammenhang mit der API der Library of Congress drei ganz grundlegende Strategien kennengelernt, wie die Anzahl der Anfragen über das Paket requests begrenzt werden kann. Dabei habt ihr auch gesehen, wie die Anzahl der Anfragen im Gesamten limitiert werden kann, wenn aus mehreren Codeabschnitten heraus Anfragen gestellt werden. Eine einfache Lösung, die mit allen drei besprochenen Strategien umsetzbar ist, ist die Definition einer generischen Funktion, über die alle Anfragen von überall im Code geleitet werden. Der folgende Code ergänzt dieses Vorgehen um weitere Aspekte: die Anfragen werden aus einer requests-Session heraus gestellt, für Statuscodes im 400er und 500er Bereich wird mit raise_for_status() automatisch eine Ausnahme geworfen, die dann im Except-Block behandelt wird, und mit dem Parameter timeout wird angegeben, wieviele Sekunden auf eine Antwort vom Server gewartet werden soll, bevor eine Ausnahme geworfen wird:</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ratelimit</span><span class="w"> </span><span class="kn">import</span> <span class="n">limits</span><span class="p">,</span> <span class="n">sleep_and_retry</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">PERIOD</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">CALLS_PER_PERIOD</span> <span class="o">=</span> <span class="mi">80</span>
<span class="n">TIMEOUT</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Alternativ kann mit TIMEOUT = (3, 8) zwischen Verbindungsaufbau (connect timeout) und Lesen der Antwort (read timeout) unterschieden werden</span>
<span class="n">USER_AGENT</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;mein-forschungsprojekt (https://github.com/meine-seite/mein-forschungsprojekt) requests/</span><span class="si">{</span><span class="n">requests</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="nd">@sleep_and_retry</span>
<span class="nd">@limits</span><span class="p">(</span><span class="n">calls</span><span class="o">=</span><span class="n">CALLS_PER_PERIOD</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="n">PERIOD</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_page</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">TIMEOUT</span><span class="p">):</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fetched </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2"> with status code: </span><span class="si">{</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">response</span>
    <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Request for </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2"> failed with exception: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;https://www.loc.gov/pictures/search/?q=suffragettes&amp;fo=json&quot;</span>
<span class="n">num_pages</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">req_session</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">req_session</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;User-Agent&quot;</span><span class="p">:</span> <span class="n">USER_AGENT</span><span class="p">})</span>

<span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_pages</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">req_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">base_url</span><span class="si">}</span><span class="s2">&amp;sp=</span><span class="si">{</span><span class="n">page</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">get_page</span><span class="p">(</span><span class="n">req_session</span><span class="p">,</span> <span class="n">req_url</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">response</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Diesen Fall hier abfangen</span>
    <span class="c1"># Rest des Codes, s. Lösung zum Übungsblatt 9</span>
</pre></div>
</div>
</div>
</div>
<p>Diesen Code erkennt ihr vielleicht wieder: Es handelt sich um eine modifizierte Version der Musterlösung zum Übungsblatt 9.</p>
<p>Neben dem Rate Limit selbst wollt ihr aber oft auch kontrollieren, wie oft beim Scheitern einer Anfrage versucht werden soll, eine Anfrage erneut zu stellen, wieviele <strong>Retries</strong> es also geben soll, und wie lange vor jedem Retry gewartet werden soll. Das folgende Beispiel illusriert eine Möglichkeit, wie Retries im Zusammenhang mit requests konfiguriert werden können sowie weitere Möglichkeiten, das Verhalten von requests zu steuern. Dazu wird auf das Paket urllib3 zurückgegriffen, das requests selbst unter der Motorhaube nutzt. Die verwendeten Parameter werden dabei in den Kommentaren erläutert. Beachtet aber, dass Redirects, also Weiterleitungen, auch ohne den Parameter redirect=2 durch requests standardmäßig berücksichtigt werden; mit redirect=2 wird nur die maximale Anzahl der Weiterleitungen, denen gefolgt werden soll, auf 2 begrenzt.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">requests.adapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">HTTPAdapter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">urllib3.util.retry</span><span class="w"> </span><span class="kn">import</span> <span class="n">Retry</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ratelimit</span><span class="w"> </span><span class="kn">import</span> <span class="n">limits</span><span class="p">,</span> <span class="n">sleep_and_retry</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># Settings</span>
<span class="n">PERIOD</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">CALLS_PER_PERIOD</span> <span class="o">=</span> <span class="mi">80</span>
<span class="n">TOTAL_RETRIES</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">BACKOFF_FACTOR</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">STATUS_FORCELIST</span> <span class="o">=</span> <span class="p">(</span><span class="mi">429</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">502</span><span class="p">,</span> <span class="mi">503</span><span class="p">,</span> <span class="mi">504</span><span class="p">)</span>
<span class="n">TIMEOUT</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">USER_AGENT</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;mein-forschungsprojekt (https://github.com/meine-seite/mein-forschungsprojekt) requests/</span><span class="si">{</span><span class="n">requests</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Funktionsdefinitionen</span>
<span class="k">def</span><span class="w"> </span><span class="nf">setup_session</span><span class="p">(</span><span class="n">user_agent</span><span class="p">,</span> <span class="n">total_retries</span><span class="o">=</span><span class="n">TOTAL_RETRIES</span><span class="p">,</span> <span class="n">backoff_factor</span><span class="o">=</span><span class="n">BACKOFF_FACTOR</span><span class="p">,</span> <span class="n">status_forcelist</span> <span class="o">=</span> <span class="n">STATUS_FORCELIST</span><span class="p">):</span>
    <span class="n">session</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>

    <span class="n">retries</span> <span class="o">=</span> <span class="n">Retry</span><span class="p">(</span>
        <span class="n">total</span><span class="o">=</span><span class="n">total_retries</span><span class="p">,</span> <span class="c1"># Maximal fünf Retries</span>
        <span class="n">backoff_factor</span><span class="o">=</span><span class="n">backoff_factor</span><span class="p">,</span> <span class="c1"># Exponential backoff: bei jeder zusätzlichen Anfrage etwas länger warten</span>
        <span class="n">status_forcelist</span><span class="o">=</span><span class="n">status_forcelist</span><span class="p">,</span> <span class="c1"># Bei diesen Statuscodes nochmal probieren</span>
        <span class="n">allowed_methods</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;GET&quot;</span><span class="p">,),</span>
        <span class="n">redirect</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># Für Statuscodes im 300er-Bereich</span>
        <span class="n">respect_retry_after_header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Retry-After Header in der Response beachten, falls vorhanden</span>
        <span class="n">raise_on_status</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Bei Statuscodes im Bereich 400-599 wir nach der maximalen Anzahl von Retries eine Exception geworfen</span>
    <span class="p">)</span>

    <span class="n">adapter</span> <span class="o">=</span> <span class="n">HTTPAdapter</span><span class="p">(</span><span class="n">max_retries</span><span class="o">=</span><span class="n">retries</span><span class="p">)</span>
    <span class="n">session</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s2">&quot;http://&quot;</span><span class="p">,</span> <span class="n">adapter</span><span class="p">)</span> <span class="c1"># Einstellungen für alle Requests über http verwenden</span>
    <span class="n">session</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s2">&quot;https://&quot;</span><span class="p">,</span> <span class="n">adapter</span><span class="p">)</span> <span class="c1"># Einstellungen für alle Requests über https verwenden</span>

    <span class="n">session</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;User-Agent&quot;</span><span class="p">:</span> <span class="n">user_agent</span><span class="p">})</span>
    <span class="k">return</span> <span class="n">session</span>

<span class="nd">@sleep_and_retry</span>
<span class="nd">@limits</span><span class="p">(</span><span class="n">calls</span><span class="o">=</span><span class="n">CALLS_PER_PERIOD</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="n">PERIOD</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_page</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">TIMEOUT</span><span class="p">):</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fetched </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2"> with status code: </span><span class="si">{</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span>
    <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Request for </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2"> failed with exception: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="c1"># Hauptlogik mit Aufruf der Funktionen</span>
<span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;https://www.loc.gov/pictures/search/?q=suffragettes&amp;fo=json&quot;</span>
<span class="n">num_pages</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">req_session</span> <span class="o">=</span> <span class="n">setup_session</span><span class="p">(</span><span class="n">USER_AGENT</span><span class="p">)</span>

<span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_pages</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">req_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">base_url</span><span class="si">}</span><span class="s2">&amp;sp=</span><span class="si">{</span><span class="n">page</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">get_page</span><span class="p">(</span><span class="n">req_session</span><span class="p">,</span> <span class="n">req_url</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">response</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Diesen Fall hier abfangen</span>
    <span class="c1"># Rest des Codes</span>
</pre></div>
</div>
</div>
</div>
<p>Zum Verständnis der Retry-Logik sollte erwähnt werden, dass die Retry-Logik per Default nur bei Problemen bei der Anfrage selbst (Netzwerkprobleme, Timeouts,..) ausgelöst wird. Mit status_forcelist werden deswegen explizit Statuscodes angegeben, bei denen die Retry-Logik zusätzlich ausgelöst werden soll. Der Parameter respect_retry_after_header=True bewirkt, dass wenn die Antwort vom Server den Retry-After Header verwendet, die darin angegebene Wartezeit verwendet wird statt der mit backoff_factor und ratelimit selbst festgelegten Wartezeit. Diese Variante macht schon vieles besser, aber es gibt noch einige Probleme: Die Aufrufe der Funktion get_page() werden zwar durch unsere ratelimit-Dekoratoren kontrolliert, aber die Retries erfordern keinen Aufruf der Funktion get_page(). Wenn die Anfrage bereits scheitert, bevor sie den Server erreicht, z.B. aufgrund von Netzwerkproblemen, ist das okay. Aber was, wenn Anfragen für die in unserer mit <code class="docutils literal notranslate"><span class="pre">status_forcelist</span></code> angegebenen Liste von HTTP-Statuscodes wiederholt werden? Dann werden bei einem Aufruf der Funktion get_page() mehrere Anfragen an den Server gesendet und das festgelegte Ratelimit wird womöglich überschritten! Zu bedenken ist auch, dass der Scraper bei einer gescheiterten Anfrage nicht abbricht, sondern nur die URL und die Ausnahme im Log vermerkt und dann mit der nächsten URL weitermacht. Im Körper der for-Schleife, aus der heraus die get_page()-Funktion aufgerufen wird, muss also der Fall berücksichtigt werden, dass die Funktion None zurückgibt, wenn eine Anfrage scheitert. Die Seiten, die nicht erfolgreich gescraped werden konnten, sollten in einem Web Scraping Projekt aber natürlich nicht einfach verschwinden, sondern dieser Fall muss untersucht und beim Verarbeiten der Daten berücksichtigt werden. Wenn die gescrapten Daten z.B. in einem Pandas Dataframe gespeichert werden, sollten Seiten, die nicht gescraped werden konnten, also beispielsweise trotzdem vorkommen, aber die entsprechende Zeile im Dataframe ist leer, oder es könnte eine weitere Spalte hinzugefügt werden, in der der Statuscode oder Grund des Scheiterns der Anfrage vermerkt wird. Es kann auch eine eigene Datei angelegt werden, in der die URLs von Seiten, die nicht gescraped werden konnten, gesammelt werden. Wenn es allerdings wichtig ist, dass alle Seiten gescraped werden, dann sollte die get_page() Funktion dagegen lieber nicht None zurückgeben, sondern es sollte mit dem Schlüsselwort <code class="docutils literal notranslate"><span class="pre">raise</span></code> eine eigene Ausnahme geworfen werden, damit der aufrufende Code, also unsere for-Schleife, die Informationen über die Ausnahme erhält und die Ausführung abbricht.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Verständnisfragen:</p>
<ul class="simple">
<li><p>Wie könnte sichergestellt werden, dass das Rate Limit auch bei Retries eingehalten wird? Lest euch dazu den folgenden Blogbeitrag durch: <a class="reference external" href="https://rebrowser.net/blog/python-requests-retry-the-ultimate-guide-to-handling-failed-http-requests-in-python">https://rebrowser.net/blog/python-requests-retry-the-ultimate-guide-to-handling-failed-http-requests-in-python</a></p></li>
<li><p>Schreibt die Funktion get_page() so um, dass beim Scheitern einer Anfrage mithilfe des Schlüsselworts <code class="docutils literal notranslate"><span class="pre">raise</span></code> und/oder der requests-Methode raise_for_status() eine Ausnahme geworfen wird und die Ausführung abbricht.</p></li>
<li><p>Was bedeuten die Statuscodes 429, 500, 502, 503 und 504, für die im zweiten Codebeispiel explizite Retries implementiert wurden?</p></li>
</ul>
</div>
</section>
<section id="versionsverwaltung-nutzen">
<h2><span class="section-number">11.4.10. </span>Versionsverwaltung nutzen<a class="headerlink" href="#versionsverwaltung-nutzen" title="Link to this heading">#</a></h2>
<p>Im Laufe des Semesters hat sich euer Ordner mit den Notebooks bestimmt mit einer Reihe Dateien der Art “uebung_9_loesung_v1.ipynb”, “uebung_9_loesung_v2.ipynb”, “uebung_9_loesung_final.ipynb” gefüllt. Klar: Wenn ihr eine alternative Lösung gefunden habt, wollt ihr diese vielleicht nicht löschen, oder ihr wollt lieber erstmal eine Sicherheitskopie anlegen, bevor ihr den Code noch einmal umschreibt. Weil das beim Coden ein häufiges Problem ist, haben sich Systeme zur sogenannten Versionsverwaltung etabliert. In der professionellen Softwareentwicklung ist die Verwendung solcher Systeme ein unverzichtbarer Standard. Eine extrem verbreitete, kostenfreie und betriebssystemübergreifende Variante ist die Kombination von <a class="reference external" href="https://git-scm.com/doc">Git</a> mit <a class="reference external" href="https://github.com/">GitHub</a>. Die Git-Dokumentationsseiten definieren Versionsverwaltung wie folgt:</p>
<blockquote>
<div><p>Was ist “Versionsverwaltung”, und warum solltest du dich dafür interessieren? Versionsverwaltung ist ein System, welches die Änderungen an einer oder einer Reihe von Dateien über die Zeit hinweg protokolliert, sodass man später auf eine bestimmte Version zurückgreifen kann. Die Dateien, die in den Beispielen in diesem Buch unter Versionsverwaltung gestellt werden, enthalten Quelltext von Software. Tatsächlich kann in der Praxis nahezu jede Art von Datei per Versionsverwaltung nachverfolgt werden. (Quelle: <a class="reference external" href="https://git-scm.com/book/de/v2/Erste-Schritte-Was-ist-Versionsverwaltung%3F">Git Dokumentationsseiten</a>)</p>
</div></blockquote>
<p>Die mithilfe von Git lokal angelegten Git Repositories (also Ordner auf dem Computer, in denen mithilfe von Git Änderungen an Dateien nachverfolgt werden) können auf Online-Plattformen wie GitHub hochgeladen werden, um sie mit anderen zu teilen oder für die private Nutzung an zentraler Stelle zu speichern. In einem GitHub-Repository kann neben dem Code und den Daten für ein Webscraping-Projekt auch eine Datei mit den Versionen der verwendeten Python Pakete und der Python-Version geteilt werden. Das ist empfehlenswert, denn so macht ihr es anderen leichter, euren Code zu reproduzieren. Eine solche Datei könnt ihr ganz einfach in Anaconda Prompt bzw. dem Terminal generieren, indem ihr die virtuelle Umgebung aktiviert und dann eingebt <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">env</span> <span class="pre">export</span> <span class="pre">--no-builds</span> <span class="pre">&gt;</span> <span class="pre">environment.yml</span></code>. Diese Datei kann dann jemand anderes nutzen, um bei sich auf dem Computer mit dem Befehl <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">env</span> <span class="pre">create</span> <span class="pre">-f</span> <span class="pre">environment.yml</span></code> eine virtuelle Umgebung mit denselben Paketen zu erstellen.</p>
<p>Über <a class="reference external" href="https://github.com/">GitHub</a> könnt ihr auch andere Web Scraping Projekte, Python-Pakete oder einzelne Module und Funktionen, deren Einsatzmöglichkeiten ihr verstehen wollt, suchen. Gebt dazu in der Suchmaske einfach ein Stichwort, den Namen eines Pakets oder der Funktion ein, wenn ihr diesen kennt, und filtert die Ergebnisse anschließend nach der Programmiersprache. Bevor ihr euch dazu entscheidet, für eine Seite einen Web Scraper zu schreiben, könnt ihr auch erst einmal auf GitHub suchen, ob es vielleicht bereits einen Web Scraper für die Seite gibt. Aber Vorsicht! Bevor ihr fremden Code ausprobiert, lest euch den Abschnitt “Sicher scrapen” durch und trefft ggf. Vorsichtsmaßnahmen. Allgemein gilt: Wenn ein GitHub Repository viele Maintainer:innen hat und die letzten Änderungen (Commits) erst wenige Tage, Wochen oder Monate alt sind, dann handelt es sich meist um vertrauenswürdigen Code. Ein weiterer Anhaltspunkt sind die Meldungen unter dem Tab “Issues” in einem GitHub Repository. Hier findet ihr Berichte über Fehler, fehlende Features oder Probleme beim Verwenden des Codes in verschiedenen Ausführungsumgebungen.</p>
<p>Einen ausführlichen Einstieg in das Thema Versionsverwaltung mit Git und GitHub bietet der Kurs <a class="reference external" href="https://open.hpi.de/courses/git2020/">Let’s Git - Versionsverwaltung und OpenSource</a> des Hasso-Plattner-Instituts.</p>
</section>
<section id="strukturierung-von-python-projekten">
<h2><span class="section-number">11.4.11. </span>Strukturierung von Python-Projekten<a class="headerlink" href="#strukturierung-von-python-projekten" title="Link to this heading">#</a></h2>
<p>Bisher habt ihr Code immer in Jupyter Notebooks geschrieben und ausgeführt. Aber wenn ihr euch auf GitHub oder woanders auf die Suche nach fremdem Code macht, dann begegnen euch häufig auch Pythonskripte mit der Dateiendung .py und größere Projekte, die aus mehreren .py, .ipynb oder anderen Dateien bestehen. Das kann verwirrend sein. Wenn euer Webscraping Projekt komplexer wird und nicht mehr nur aus einem Jupyter Notebook, sondern noch aus Input- und Output-Dateien oder weiteren Skripten besteht, fragt ihr euch vielleicht, wie ihr die verschiedenen Dateien am besten organisieren solltet. Wir schauen uns deswegen in der letzten Stunde ein paar Beispiele an.</p>
<p>Ein wichtiges Vorwissen, um mit Python-Skripten, also Dateien mit der Dateiendung .py, zu arbeiten, ist, wie man den Code in solchen Dateien ausführen kann. Grundsätzlich ist der einfachste Weg, um ein Pythonskript auszuführen, die Eingabe des Befehls <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">name_der_datei.py</span></code> in der Kommandozeile bzw. dem Terminal. Die Kommandozeile bzw. das Terminal kann auch im Jupyterlab oder in einem sogenannten Code-Editor (-&gt; Schaubild Python Code ausführen im Kapitel 2.1 auf der Kurswebsite) aufgerufen werden. Um eine Datei auszuführen, muss entweder erst mit dem Befehl <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">pfad/zum/verzeichnis</span></code> in das Verzeichnis gewechselt werden, in dem das Python-Skript liegt, oder es muss beim Ausführen zusätzlich der Pfad zur Datei angegeben werden, also <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">pfad/zum/verzeichnis/name_der_datei.py</span></code>. Manchmal begegnet man aber auch Dateien mit der Dateiendung .py, die nicht primär als Skripte zum Ausführen im Terminal gedacht sind, sondern die für den Import in andere Python-Skripte gedacht sind (-&gt; Kapitel 4.2 auf der Kurswebsite). Manchmal, aber nicht immer, kann man ein solches für den Import gedachtes Skript daran erkennen, dass im selben Verzeichnis noch eine Datei mit dem Namen <code class="docutils literal notranslate"><span class="pre">__init__.py</span></code> liegt. Um so eine Datei auszuführen, muss die zusätzliche Option (auch “Flag” genannt) <code class="docutils literal notranslate"><span class="pre">-m</span></code> beim Ausführen mit angegeben werden, und das Verzeichnis, in dem sich die Datei befindet, kann mit der Notation <code class="docutils literal notranslate"><span class="pre">name_des_verzeichnisses.name_der_datei</span></code> angegeben werden. Der vollständige Befehl lautet dann <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">name_des_verzeichnisses.name_der_datei</span></code>.</p>
<p>Zum Nachlesen empfehle ich Kapitel 24: “Module Packages” und 25: “Module Odds and Ends” in <a class="reference external" href="https://www.oreilly.com/library/view/learning-python-6th/9781098171292/">Martin Lutz (2025), Learning Python</a>. Darin findet ihr auch eine sehr gute Erläuterung zur Verwendung des Idioms <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">__name__</span> <span class="pre">==</span> <span class="pre">&quot;__main__&quot;</span></code>, das euch begegnen wird, wenn ihr nach Pythonskripte von anderen sucht. Grundsätzlich wird dieses Konstrukt immer dann verwendet, wenn ermöglicht werden soll, dass der Code sowohl in anderen Dateien importiert werden kann, als auch in der Kommandozeile ausgeführt werden kann.</p>
</section>
<section id="webscraping-projekte-mit-mehreren-datenquellen">
<h2><span class="section-number">11.4.12. </span>Webscraping-Projekte mit mehreren Datenquellen<a class="headerlink" href="#webscraping-projekte-mit-mehreren-datenquellen" title="Link to this heading">#</a></h2>
<p>Wenn ihr für ein Projekt nicht nur die Daten von einer, sondern von mehreren Seiten scrapen wollt und das Scrapen mehrere Schritte erfordert, wird euer Code schnell unübersichtlich und das Ausführen in einem Jupyter Notebook wird unpraktisch: Sehr langer Output wird in Jupyterlab abgekürzt dargestellt, was ein Debuggen erschwert, und wenn Codezellen manuell ausgeführt werden sollen, muss sehr darauf geachtet werden, dass die Reihenfolge eingehalten und keine Codezelle vergessen wird. Für größere Web Scraping-Projekte mit mehreren Datenquellen und verschiedenen Verarbeitungsschritten lohnt sich deswegen der Umstieg von Jupyter Notebooks auf Pythonskripte, damit nicht alles in einer Datei steht. Neben einer solchen Modularisierung profitieren größere Webscraping-Projekte auch von zusätzlichen Mechanismen wie dem bereits besprochenen Logging, einer ordentlichen Ausnahmebehandlung, aber auch Caching und, bei mehreren Datenquellen, einer nicht-sequentiellen Ausführung des Codes. Was bedeutet das? Wenn Daten von zwei verschiedenen Websites sequentiell gescraped werden, heißt das, dass erst die Daten von der einen Website und dann die Daten von der zweiten Website gescraped werden. Wenn beim Scrapen großzügige Wartezeiten eingebaut werden, um die Rate Limits der Webseite einzuhalten, dann ist das ziemlich ineffizient, weil viel Wartezeit anfällt, während der nichts passiert. Bei einer nicht-sequentiellen Ausführung ist das anders. Die sogenannte nebenläufige Ausführung von Code ermöglicht, die anfallende Wartezeit auszunutzen, indem während der Wartezeit bereits andere Verarbeitungsschritte erfolgen und zum Beispiel Anfragen an die zweite Website gestellt werden. Bei einer parallelen Ausführung von Code werden die Anfragen an verschiedene Webseiten sogar gleichzeitig ausgeführt.</p>
<p>Eine ordentliche Implementierung von Caching und nicht-sequentielle Programmausführung erfordert aber fortgeschrittene Programmierkenntnisse. Es gibt deswegen einige Pakete, die Programmbausteine unter anderem für nebenläufige Netzwerkanfragen und Caching bereitstellen. Zwei solche Pakete sind in Python <a class="reference external" href="https://www.scrapy.org/">Scrapy</a> und <a class="reference external" href="https://docs.prefect.io/v3/get-started">Prefect</a>.</p>
<p>Zum Einstieg in Scrapy empfehle ich das <a class="reference external" href="https://docs.scrapy.org/en/latest/intro/tutorial.html">Scrapy-Tutorial</a>, in dem die Zitate von der euch bereits bekannten Seite <a class="reference external" href="http://quotes.toscrape.com">quotes.toscrape.com</a> extrahiert werden, und für den Einstieg in Prefect empfehle ich das <a class="reference external" href="https://docs.prefect.io/v3/examples/simple-web-scraper">Beispiel</a> aus den Prefect Dokumentationsseiten, welches illustriert, wie Prefect für Webscraping verwendet werden kann.</p>
</section>
<section id="sicher-scrapen">
<h2><span class="section-number">11.4.13. </span>Sicher Scrapen<a class="headerlink" href="#sicher-scrapen" title="Link to this heading">#</a></h2>
<p>Die Anforderungen an Sicherheit und Anonymität können sich von Web Scraping Projekt zu Web Scraping Projekt unterscheiden. Ein paar Dinge sind aber allgemein empfehlenswert:</p>
<ul class="simple">
<li><p>Achtet nach Möglichkeit darauf, dass in eurer Anfrage-URL https statt http steht.</p></li>
<li><p>Falls ihr euch auf einer Webseite anmelden müsst, um Inhalte scrapen zu können, solltet ihr immer ein Profil speziell für diesen Zweck erstellen. Gebt niemals sensible Passwörter in den automatisierten Chrome Browser ein.</p></li>
<li><p>Seid außerdem vorsichtig, wenn ihr verschiedene Tipps online lest: Bevor ihr einfach komplexen Code von Stackoverflow oder einem ähnlichen Forum kopiert, recherchiert, ob es nicht vielleicht einen einfacheren Weg gibt, um euer Problem zu lösen. Denn wenn ihr gar nicht versteht, was ihr gerade macht, könnt ihr auch nicht abschätzen, welche Risiken das kopierte Vorgehen mit sich bringt, und welche Sicherheitsvorkehrungen die andere Person vielleicht vorher getroffen hat.</p></li>
<li><p>Dasselbe gilt für Code von KI-Bots, Code aus GitHub Repositories und sogar für Python Pakete: Schaut am besten immer selbst erst den Code an, bevor ihr ihn ausführt, und recherchiert erst einmal, ob der Code aktuell ist, ob es sich um ein bekanntes und gut gepflegtes Paket handelt oder ob es ein acht Jahre altes Hobbyprojekt von einer Einzelperson ist. Diese Informationen findet ihr meist über die Projekt-Links auf der Seite des Pakets auf <a class="reference external" href="http://pypi.org">pypi.org</a> oder direkt auf GitHub.</p></li>
<li><p>Beachtet an dieser Stelle auch noch einmal den Sicherheitshinweis aus der ersten Stunde: Code, den ChatGPT oder eine andere LLM-Anwendung generiert hat zu kopieren und auf dem eigenen Rechner auszuführen, ohne zu verstehen, was der Code macht, birgt erhebliche Sicherheitsrisiken, besonders dann, wenn aus dem Code heraus Webseiten aufgerufen werden. Dazu zählt nicht nur die Tatsache, dass Code, der aus Internetforen kopiert oder von LLMs generiert wird, <a class="reference external" href="https://doi.org/10.48550/arXiv.2403.15600">oft sicherheitsrelevante Schwachstellen aufweist</a>, sondern auch Risiken durch neuere Angriffsstrategien wie das sogenannte <a class="reference external" href="https://hackernoon.com/ai-slopsquatting-how-llm-hallucinations-poison-your-code">AI Slopsquatting</a>, <a class="reference external" href="https://www.ibm.com/think/topics/data-poisoning">data-, model- und tool poisoning</a> sowie <a class="reference external" href="https://www.ibm.com/de-de/topics/prompt-injection">indirect prompt injections</a>. Solche Angriffe zählen zu “Supply Chain Attacks”, die in den letzten Jahren massiv angestiegen sind und insbesondere Open Source Projekte im Python- und JavaScript Ökosystem betreffen (siehe dazu z.B. <a class="reference external" href="https://medium.com/&#64;ronityadav234/supply-chain-malware-targets-popular-npm-pypi-packages-in-gluestack-attack-5cec43cf2a6a">diesen Blogbeitrag</a>).</p></li>
<li><p>Bösartiger Code findet sich auch sehr häufig in Paketen, die sehr ähnlich benannt sind wie populäre Bibliotheken und häufige Schreibfehler abbilden (beatifulsoup4, beautifulsoup o.Ä. statt beautifulsoup4). Diese Form der Angriffe wird deswegen auch <a class="reference external" href="https://www.security-insider.de/check-point-entdeckt-typosquatting-angriffe-bei-pypi-a-84eb4b72a9bfeb31e3d55c1c4070e1f8/">Typosquatting</a> genannt. Achtet deswegen immer darauf, dass ihr beim Installieren die Paketnamen richtig schreibt.</p></li>
<li><p>Es empfiehlt sich außerdem, beim Web Scrapen ganz allgemein einen Useraccount auf eurem Computer zu verwenden, der keine Administratorrechte hat. Falls doch etwas schief gehen sollte, ist dadurch der Schaden zumindest begrenzt.</p></li>
<li><p>Wenn ihr ganz sicher gehen wollt, verwendet eine <a class="reference external" href="https://de.wikipedia.org/wiki/Virtuelle_Maschine">virtuelle Maschine</a> zum Scrapen. Virtuelle Maschinen können z.B. mithilfe spezialisierter Software wie <a class="reference external" href="https://mac.getutm.app/">UTM</a> für MacOS erstellt werden.</p></li>
<li><p>In manchen der Praxisbeispiele wurden Passwörter und Usernamen zu Demonstrationszwecken in den Code geschrieben. Beachtet aber die Hinweise dazu und macht das nicht so nach. Zugangsdaten sollten niemals direkt in den Code geschrieben werden.</p></li>
<li><p>Mit dem Aktivitätsmonitor (MacOS) bzw. Ressourcenmonitor (Windows) könnt ihr nach dem Scrapen überprüfen, ob alle Python- und Chromedriverprozesse korrekt beendet wurden.</p></li>
<li><p>Je nach Web Scraping Projekt kann es empfehlenswert sein, die eigene IP-Adresse mithilfe von VPN oder Proxy zu verstecken. Allerdings ist das ethisch und rechtlich eine schwierige Sache: Wir haben beispielsweise bereits besprochen, dass man sich strafbar machen kann, wenn man beim Web Scraping blockiert wird und anstatt seinen Web Scraper robots.txt-konform umzuschreiben einfach seine IP-Adresse versteckt. Hier ist also Vorsicht und Eigenrecherche geboten.</p></li>
</ul>
</section>
<section id="effizienter-scrapen-mit-sitemaps">
<h2><span class="section-number">11.4.14. </span>Effizienter Scrapen mit Sitemaps<a class="headerlink" href="#effizienter-scrapen-mit-sitemaps" title="Link to this heading">#</a></h2>
<p>Eine Sitemap ist eine strukturierte Übersicht über alle Unterseiten einer Website. Webseitenbetreiber:innen stellen Sitemaps zur Verfügung, damit beim Crawlen der Seite durch Web Crawler verschiedener Suchmaschinen alle Unterseiten einfach gefunden werden können. Je nach gesuchten Daten können Sitemaps auch beim Web Scraping eingesetzt werden, um effizienter eine Liste von zu scrapenden Unterseiten zusammenzustellen. Sitemaps werden meist im XML-Format bereitgestellt, das heißt, dass zur Suche in solchen Sitemaps ebenfalls XPath verwendet werden kann. Sitemaps sind oft unter der Adresse <a class="reference external" href="http://www.beispielwebseite.com/sitemap.xml">www.beispielwebseite.com/sitemap.xml</a> erreichbar, aber anders als bei der robots.txt gibt es noch einige andere typische Adressen. Eine Übersicht über solche typischen Sitemap-Adressen findet ihr <a class="reference external" href="https://seocrawl.com/en/how-to-find-a-sitemap/">hier</a>. Manche Webseitenbetreiber:innen verlinken die Sitemap zudem in der robots.txt. Dies ist zum Beispiel bei der Website <a class="reference external" href="http://realpython.com">realpython.com</a> der Fall. Die Sitemap ist hier unter <a class="reference external" href="https://realpython.com/robots.txt">https://realpython.com/robots.txt</a> verlinkt und hat die Adresse <a class="reference external" href="https://realpython.com/sitemap.xml">https://realpython.com/sitemap.xml</a>. Die Sitemap selbst sieht so aus:</p>
<figure class="align-default" id="id247">
<a class="bg-transparent reference internal image-reference" href="../../../_images/realpython_sitemap.png"><img alt="Realpython Sitemap" class="bg-transparent" src="../../../_images/realpython_sitemap.png" style="width: 75%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 11.15 </span><span class="caption-text">Sitemap der Website <a class="reference external" href="http://realpython.com">realpython.com</a></span><a class="headerlink" href="#id247" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="wo-steht-nochmal">
<h2><span class="section-number">11.4.15. </span>Wo steht nochmal…<a class="headerlink" href="#wo-steht-nochmal" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>F-Strings: <a class="reference external" href="https://lipogg.github.io/webscraping-mit-python/chapters/08/subchapters/02_dracor_api.html#f-strings">Kapitel 8.2.3</a></p></li>
<li><p>List Comprehension: <a class="reference external" href="https://lipogg.github.io/webscraping-mit-python/chapters/08/subchapters/02_dracor_api.html#list-comprehension">Kapitel 8.2.3</a></p></li>
<li><p>Daten schreiben: <a class="reference external" href="https://lipogg.github.io/webscraping-mit-python/chapters/08/subchapters/02_dracor_api.html#daten-schreiben">Kapitel 8.2.3</a> und <a class="reference external" href="https://lipogg.github.io/webscraping-mit-python/chapters/10/subchapters/01_pandas_bereinigen.html#daten-zwischenspeichern">Kapitel 10.1.3</a></p></li>
<li><p>Rate Limiting: <a class="reference external" href="https://lipogg.github.io/webscraping-mit-python/chapters/08/subchapters/03_loc_api.html#rate-limits-berucksichtigen-und-die-abfragerate-steuern">Kapitel 8.3.5</a></p></li>
<li><p>XPath: <a class="reference external" href="https://lipogg.github.io/webscraping-mit-python/chapters/09/subchapters/03_fortsetzung_selenium.html#scrollvorgang-und-mausklick-simulieren">Kapitel 9.3.3</a> und <a class="reference external" href="https://lipogg.github.io/webscraping-mit-python/chapters/09/subchapters/03_fortsetzung_selenium.html#elemente-finden">Kapitel 9.3.5</a></p></li>
</ul>
</section>
<section id="don-t-do-this">
<h2><span class="section-number">11.4.16. </span>Don’t do this…<a class="headerlink" href="#don-t-do-this" title="Link to this heading">#</a></h2>
<p>Zum Schluss möchte ich euch noch eine kleine Empfehlung mit auf den Weg geben: Nicht immer ist Web Scraping wirklich die schnellste und effizienteste Lösung für euer Problem. Es macht wahrscheinlich mehr Spaß, Code zu schreiben, als direkt die Aufgabe anzugehen, aber wenn ihr unbedingt eine Deadline einhalten müsst, dann ist es natürlich auch okay, ein Web Scraping Projekt abzubrechen und eure Aufgabe einfach manuell zu lösen. Also in diesem Sinne:</p>
<figure class="align-default" id="id248">
<a class="bg-transparent reference internal image-reference" href="../../../_images/automation_xkcd.png"><img alt="Comic zur Warnung" class="bg-transparent" src="../../../_images/automation_xkcd.png" style="width: 75%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 11.16 </span><span class="caption-text">Don’t do this… Quelle: <a class="reference external" href="https://www.scrapingbee.com/blog/selenium-python/">Kevin Sahin 2022</a></span><a class="headerlink" href="#id248" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="quellen">
<h2><span class="section-number">11.4.17. </span>Quellen<a class="headerlink" href="#quellen" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id1">
<ol class="arabic simple" start="1">
<li id="id202"><p>Peter Ernesti, Johannes und Kaiser. Python 3: Ausnahmebehandlung. 2020. URL: <a class="reference external" href="https://openbook.rheinwerk-verlag.de/python/22_001.html">https://openbook.rheinwerk-verlag.de/python/22_001.html</a>.</p></li>
<li id="id206"><p>Ognian Mikov. How to Find the Sitemap of a Website. 2021. URL: <a class="reference external" href="https://seocrawl.com/en/how-to-find-a-sitemap/">https://seocrawl.com/en/how-to-find-a-sitemap/</a>.</p></li>
<li id="id192"><p>Ryan Mitchell. <em>Webscraping with Python. Collecting More Data from the Modern Web</em>. O'Reilley, Farnham et al., 2018.</p></li>
<li id="id205"><p>Jonathan Mondaut. Take Advantage of Sitemaps for Efficient Web Scraping: A Comprehensive Guide. 2023. URL: <a class="reference external" href="https://medium.com/&#64;jonathanmondaut/take-advantage-of-sitemaps-for-efficient-web-scraping-a-comprehensive-guide-c35c6efe52d3">https://medium.com/&#64;jonathanmondaut/take-advantage-of-sitemaps-for-efficient-web-scraping-a-comprehensive-guide-c35c6efe52d3</a>.</p></li>
<li id="id195"><p>Kenneth Reitz. Requests Documentation: Errors and Exceptions. 2023. URL: <a class="reference external" href="https://requests.readthedocs.io/en/latest/user/quickstart/?highlight=cookie#errors-and-exceptions">https://requests.readthedocs.io/en/latest/user/quickstart/?highlight=cookie#errors-and-exceptions</a>.</p></li>
<li id="id196"><p>Kevin Sahin. Web Scraping Using Selenium and Python. 2022. URL: <a class="reference external" href="https://www.scrapingbee.com/blog/selenium-python/">https://www.scrapingbee.com/blog/selenium-python/</a>.</p></li>
<li id="id193"><p>Carsten Sinz. Vorlesung &quot;Algorithmen und Datenstrukturen&quot;. 03: Korrektheit von Algorithmen, Asymptotische Laufzeiten. 2019. URL: <a class="reference external" href="https://youtu.be/7vUGHpbQABk?si=8FsyfJkPNQTjhZv-">https://youtu.be/7vUGHpbQABk?si=8FsyfJkPNQTjhZv-</a>.</p></li>
<li id="id198"><p>Baiju Muthukadan. Selenium with Python. Waits. 2024. URL: <a class="reference external" href="https://selenium-python.readthedocs.io/waits.html">https://selenium-python.readthedocs.io/waits.html</a>.</p></li>
<li id="id203"><p>Python 3.11.3 Documentation. Built-in Exceptions. URL: <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#bltin-exceptions">https://docs.python.org/3/library/exceptions.html#bltin-exceptions</a>.</p></li>
<li id="id201"><p>Python 3.11.3 Documentation. Compound Statements: The Try Statement. URL: <a class="reference external" href="https://docs.python.org/3/reference/compound_stmts.html#the-try-statement">https://docs.python.org/3/reference/compound_stmts.html#the-try-statement</a>.</p></li>
<li id="id200"><p>Python 3.11.3 Documentation. Errors and Exceptions. URL: <a class="reference external" href="https://docs.python.org/3/tutorial/errors.html">https://docs.python.org/3/tutorial/errors.html</a>.</p></li>
<li id="id204"><p>Said van de Klundert. Python Exceptions: An Introduction. URL: <a class="reference external" href="https://realpython.com/python-exceptions/">https://realpython.com/python-exceptions/</a>.</p></li>
<li id="id194"><p>Selenium 4 Documentation. Browser Options: pageLoadStrategy. 2022. URL: <a class="reference external" href="https://www.selenium.dev/documentation/webdriver/drivers/options/#pageloadstrategy">https://www.selenium.dev/documentation/webdriver/drivers/options/#pageloadstrategy</a>.</p></li>
<li id="id197"><p>Selenium 4.10 Documentation. Waits. 2023. URL: <a class="reference external" href="https://www.selenium.dev/documentation/webdriver/waits/">https://www.selenium.dev/documentation/webdriver/waits/</a>.</p></li>
<li id="id199"><p>W3Docs. How to Find an Element by CSS Class Name with XPath. 2023. URL: <a class="reference external" href="https://www.w3docs.com/snippets/css/how-to-find-an-element-by-css-class-name-with-xpath.html">https://www.w3docs.com/snippets/css/how-to-find-an-element-by-css-class-name-with-xpath.html</a>.</p></li>
</ol>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters/11/subchapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="03_apis_tokens.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11.3. </span>Authentifizierung, Autorisierung und APIs</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#user-agent-im-http-header-setzen">11.4.1. User Agent im HTTP-Header setzen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selenium-im-headless-mode">11.4.2. Selenium im Headless Mode</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#http-content-negotiation-und-linked-data">11.4.3. HTTP Content Negotiation und Linked Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regulare-ausdrucke">11.4.4. Reguläre Ausdrücke</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fehler-ausnahmen-und-ausnahmebehandlung">11.4.5. Fehler, Ausnahmen und Ausnahmebehandlung</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logging-statt-print-verwenden">11.4.6. Logging statt print() verwenden</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verschiedene-losungen-vergleichen">11.4.7. Verschiedene Lösungen vergleichen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-testen">11.4.8. Code testen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#robuster-scrapen-mit-requests">11.4.9. Robuster scrapen mit requests</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#versionsverwaltung-nutzen">11.4.10. Versionsverwaltung nutzen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strukturierung-von-python-projekten">11.4.11. Strukturierung von Python-Projekten</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#webscraping-projekte-mit-mehreren-datenquellen">11.4.12. Webscraping-Projekte mit mehreren Datenquellen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sicher-scrapen">11.4.13. Sicher Scrapen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#effizienter-scrapen-mit-sitemaps">11.4.14. Effizienter Scrapen mit Sitemaps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wo-steht-nochmal">11.4.15. Wo steht nochmal…</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#don-t-do-this">11.4.16. Don’t do this…</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quellen">11.4.17. Quellen</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lisa Poggel
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>