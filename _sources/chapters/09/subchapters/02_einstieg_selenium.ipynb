{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Einstieg Selenium"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In dieser Stunde schauen wir uns zum Einstieg wieder die Seite mit den Zitaten an. Allerdings widmen wir uns der Version der Seite https://quotes.toscrape.com/js/. Es handelt sich dabei um eine Version der Seite, bei der die Zitate mithilfe von JavaScript dargestellt werden. Zum Einstieg führen wir ein kleines Experiment durch. Öffnet euer Jupyter Notebook aus dem Abschnitt \"Einstieg BeautifulSoup\". Ersetzt URL = \"https://quotes.toscrape.com\" durch URL = \"https://quotes.toscrape.com/js\" und führt alle Codezellen aus. Was passiert?\n",
    "\n",
    "Woran liegt das? Die Inhalte der Seite werden offensichtlich nicht vorgeladen, sondern sie werden erst beim Seitenaufruf ergänzt. Das könnt ihr nachvollziehen, indem ihr die Seite im Browser aufruft, die Entwicklertools öffnet, JavaScript deaktiviert, und die Seite https://quotes.toscrape.com/js/ neu ladet. Welche Inhalte der Seite werden beim neu Laden angezeigt? Wie ist das im Vergleich mit der Seite https://quotes.toscrape.com/?\n",
    "\n",
    "### Was ist Selenium?\n",
    "\n",
    "Zum Scrapen solcher Inhalte können wir BeautifulSoup nicht verwenden, weil BeautifulSoup mit solchen mithilfe von JavaScript dargestellten Inhalten nicht umgehen kann: In unserem quotes.toscrape-Webscraper haben wir ja nur eine HTTP-Anfrage gestellt, und diese Anfrage liefert nur die HTML-Dokumente, die bereits auf dem Server liegen, bevor wir die Webseite aufrufen. Wir müssen also irgendwie an die Inhalte kommen, welche mithilfe von JavaScript im Client nachträglich eingefügt werden.\n",
    "\n",
    "Dazu gibt es verschiedene spezialisierte Python-Pakete und Bibliotheken, am gängigsten sind Selenium, die leichtgewichtigere Selenium-Variante Helium sowie Puppeteer und Playwright. Daneben gibt es auch Splash, das in Kombination mit Scrapy, eine BeautifulSoup-Alternative, verwendet wird. Das wohl meist verwendete Paket zum Scrapen dynamischer Inhalte war im letzten Jahrzehnt immer Selenium, allerdings wird es vermehrt durch Playwright verdrängt. Zum Verständnis von Playwright müssten wir uns aber noch ein anderes Netzwerkprotokoll ansehen, nämlich WebSocket. Im Rahmen dieses Seminars werden wir deswegen bei HTTP bleiben und lernen anstelle von Playwright das trotz der Konkurrenz immer noch etablierte Selenium kennen.\n",
    "\n",
    "Aber was ist denn jetzt Selenium?\n",
    "\n",
    "> Selenium automates browsers. That's it! What you do with that power is entirely up to you. Primarily it is for automating web applications for testing purposes, but is certainly not limited to just that.\n",
    "\n",
    "Quelle: [Selenium 2023](https://www.selenium.dev/)\n",
    "\n",
    "Selenium ist ein Projekt, das verschiedene Bibliotheken in verschiedenen Programmiersprachen umfasst und ursprünglich zum Testen von Webseiten entwickelt wurde. **Ganz allgemein ermöglich es Selenium, Webbrowser automatisiert zu steuern und mit ihnen zu interagieren. Das heißt, dass wir im Rahmen von Web Scraping mithilfe von Selenium auf die gerenderte Version einer Webseite zugreifen und direkt aus unserem Python-Notebook heraus verschiedene Aktionen im Webbrowser ausführen können, z.B. das Öffnen von URLs, Suchen nach und Klicken auf Elemente, Ausfüllen von Formularen und Extrahieren von Daten.** Um Selenium verwenden zu können, brauchen wir zusätzlich einen Browser und einen Treiber für diesen Browser (=Webdriver). In unserem Beispiel heute werden wir Selenium im Zusammenhang mit Chrome und dem [Chrome WebDriver](https://sites.google.com/chromium.org/driver/) verwenden. Der Chrome WebDriver stellt die Verbindung zum Chrome-Browser her und ermöglicht die Kommunikation zwischen Selenium und dem Browser.\n",
    "\n",
    "\n",
    "### Beispiel: Zitate mit Python Selenium scrapen\n",
    "\n",
    "Als Beispiel sehen wir uns zunächst wieder den Code von jemand anderem an, nämlich von [Lewis Kori](https://lewiskori.com/blog/beginner-s-guide-to-web-scraping-with-python-s-selenium/).\n",
    "\n",
    "Wir nehmen allerdings zwei Änderungen an dessen Code vor: Wir installieren den ChromeDriver nicht manuell, sondern mithilfe des Pakets webdriver-manager. Dadurch müssen wir beim Aufruf der Funktion Chrome() aus dem Paket selenium nicht manuell den Pfad zum installierten ChromeDriver angeben, sondern der Pfad wird aufomatisch eingesetzt.\n",
    "\n",
    "Außerdem verwenden wir den Präfix https anstelle von http für die quotes.toscrape Website, um eine verschlüsselte Verbindung zu der Seite herzustellen.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} selenium\n",
    "# !conda install --yes --prefix {sys.prefix} webdriver-manager"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !conda list # selenium version 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code für Python-selenium Version 3: https://github.com/SergeyPirogov/webdriver_manager\n",
    "# from selenium import webdriver\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import pandas as pd\n",
    "\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pages = 11\n",
    "\n",
    "# total = []\n",
    "# for page in range(1,pages):\n",
    "\n",
    "#     url = \"https://quotes.toscrape.com/js/page/\" + str(page) + \"/\"\n",
    "\n",
    "#     driver.get(url)\n",
    "\n",
    "#     quotes = driver.find_elements_by_class_name(\"quote\")\n",
    "#     for quote in quotes:\n",
    "#         quote_text = quote.find_element_by_class_name('text').text[1:-2]\n",
    "#         author = quote.find_element_by_class_name('author').text\n",
    "#         new = ((quote_text,author))\n",
    "#         total.append(new)\n",
    "\n",
    "# driver.close()\n",
    "# df = pd.DataFrame(total,columns=['quote','author'])\n",
    "# df.to_csv('quoted.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zur Übung sollt ihr euch im Rest der Stunde selbst erarbeiten, was dieses Skript macht. Schaut dazu in die Dokumentationsseiten zum Selenium-Python-Paket:  https://selenium-python.readthedocs.io/. Kommentiert jede Zeile, damit ihr euch später noch an die Ergebnisse eurer Recherche erinnern könnt: Diese Vorarbeit braucht ihr für die Übungsaufgabe!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quellen\n",
    "\n",
    "```{bibliography}\n",
    "   :list: enumerated\n",
    "   :filter: keywords % \"selenium\"\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
