{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Beispiel 2: Library of Congress API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Die Library of Congress (LOC) bietet eine Reihe sehr gut dokumentierter APIs zur Abfrage von Metadaten, Dateien und Volltexten aus dem Bestand der Bibliothek. Eine davon ist die API der Sammlung US-amerikanischer historischer Zeitungen Chronicling America. Diese API werden wir in dieser Stunde kennenlernen.\n",
    "\n",
    "- Übersicht über die LOC APIs: https://guides.loc.gov/digital-scholarship/accessing-digital-materials#s-lib-ctab-26648178-2\n",
    "- Dokumentation zur Chronicling America API: https://chroniclingamerica.loc.gov/about/api/\n",
    "\n",
    "Zunächst machen wir uns mit der Chronicling America API vertraut. Welche Daten können darüber abgefragt werden?\n",
    "\n",
    "Für unser Beispiel werden wir die Volltexte zu allen Ergebnissen einer Suche nach Schlüsselwörtern in den Volltexten der Zeitungen abfragen und herunterladen (Abschnitt \"Searching the directory and newspaper pages using OpenSearch\"). Die Volltexte sind mithilfe von OCR-Verfahren erstellt, also mithilfe von automatischer Bilderkennung. Unsere Suchabfrage liefert also nur diejenigen Zeitungen, in denen die Suchwörter korrekt erkannt wurden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Vorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# wir müssen zunächst die Anaconda Einstellungen ändern, damit wir das Paket ratelimit installieren können:\n",
    "# https://stackoverflow.com/questions/48493505/packagesnotfounderror-the-following-packages-are-not-available-from-current-cha\n",
    "# import sys\n",
    "# !conda config --append channels conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paket ratelimit installieren\n",
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} ratelimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T10:33:16.414953Z",
     "start_time": "2024-12-16T10:33:16.406022Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pakete importieren\n",
    "import requests\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from limits import strategies, storage, parse\n",
    "# Dieses Modul müsst ihr nicht importieren\n",
    "from utils import skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Exploration der Chronicling America API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wie in der letzten Stunde müssen wir zur Abfrage von Daten wieder eine URI nach den Vorgaben der API Dokumentation zusammensetzen.\n",
    "\n",
    "Suchabfragen können mit einem `?` an die URL https://chroniclingamerica.loc.gov/search/pages/results/ angefügt werden.\n",
    "Es gibt laut [Dokumentationsseite](https://chroniclingamerica.loc.gov/about/api/) drei verschiedene Abfrageparameter:\n",
    "\n",
    "- andtext: the search query\n",
    "- format: 'html' (default), or 'json', or 'atom' (optional)\n",
    "- page: for paging results (optional)\n",
    "\n",
    "Diese Parameter werden wir uns der Reihe nach ansehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Parameter andtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Der andtext Parameter: Volltexte nach Schlagwörtern oder Phrasen durchsuchen\n",
    "# Suche nach Schlagwörtern book AND review\n",
    "url_1 = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=book+review\"\n",
    "url_2 = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=book%20review\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Eine aufmerksame Betrachtung der Ergebnisse der Suche nach einem Suchbegriff über die Suchmaske der Website https://chroniclingamerica.loc.gov zeigt, dass die Suche über die Suchmaske genau dieselben Ergebnisse liefert wie die API-Abfrage. Das ist nicht erstaunlich, denn die URL, die beim Verwenden der Suchmaske generiert wird, ist dieselbe URL, die auch über die API abgefragt werden kann. Es handelt sich also um diesele URL, die wir für die API-Abfrage erstellen, auch, wenn die Suchparameter zum Teil etwas anders aussehen:\n",
    "\n",
    ":::{figure-md}\n",
    "<img src=\"loc_ca_suche.png\" alt=\"Suchmaske Chronicling America\" class=\"bg-transparent\" width=\"80%\">\n",
    "\n",
    "Einfache Suche über die Suchmaske der Seite Chronicling America.\n",
    ":::\n",
    "\n",
    "Wir können diese URL nutzen, um den andtext-Parameter besser zu verstehen. Wenn wir in der einfachen Suche nach dem Suchbegriff \"book review\" suchen, dann steht in der URL \"book+review\". Wenn wir stattdessen die Erweiterte Suche (Tab Advanced Search) verwenden und nach einer Phrase suchen, dann steht in der URL der Zusatz \"&phrasetext=book+review\":\n",
    "\n",
    ":::{figure-md}\n",
    "<img src=\"loc_ca_erweiterte_suche.png\" alt=\"Erweiterte Suche Chronicling America\" class=\"bg-transparent\" width=\"80%\">\n",
    "\n",
    "Erweiterte Suche über die Suchmaske der Seite Chronicling America.\n",
    ":::\n",
    "\n",
    "Tatsächlich akzeptiert auch die Chronicling America API eine Abfrage-URI mit dem Zusatz &phrasetext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Suche nach Phrase \"book review\"\n",
    "url_3 = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review\"\n",
    "# woher weiß ich das: manuelle Suche über \"advanced search\" und URL untersuchen\n",
    "# https://chroniclingamerica.loc.gov/search/pages/results/?dateFilterType=yearRange&date1=1770&date2=1963&language=&ortext=&andtext=&phrasetext=book+review&proxtext=&proxdistance=5&rows=20&searchType=advanced\n",
    "search_results = requests.get(url_3)\n",
    "# search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Parameter format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Der format-Parameter: Ergebnisse im JSON-Format abfragen\n",
    "# https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\n",
    "# erste Seite der Suchergebnisse: 20 Ergebnisse je Seite\n",
    "url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "search_results = requests.get(url)\n",
    "# search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "```{note}\n",
    "JSON im Chrome Browser ansehen\n",
    "\n",
    "Zur Ansicht der JSON-Datei im Chrome Browser können wir wieder auf die Entwicklertools zurückgreifen. Die Standardansicht ist nämlich sehr schwer lesbar, weil der JSON-String nicht formatiert ist. Um eine formatierte Ansicht zu erhalten, befolgt die folgenden Schritte: Entwicklertools öffnen -> \"Sources\"-Tab auswählen-> Link anklicken\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Parameter page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Der page Parameter: Nur die ausgewählte Ergebnisseite abfragen\n",
    "first_page = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json&page=1\"\n",
    "first_page_results = requests.get(first_page)\n",
    "# first_page_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Per Default werden immer die ersten 20 Suchergebnisse (also die erste Seite der Suchergebnisse) ausgegeben, wenn der page-Parameter weggelassen wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "default_page = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "default_page_results = requests.get(default_page)\n",
    "first_page_results.content == default_page_results.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "`````{admonition} Verständnisfragen\n",
    ":class: tip\n",
    "\n",
    "- Welche weiteren Parameter akzeptiert die Chronicling America API? Verwendet die Suchmaske unter dem [Tab Advanced Search](https://chroniclingamerica.loc.gov/search/pages/results/#tab=tab_advanced_search) und beobachtet, wie verschiedene Parameter in die URL eingefügt werden.\n",
    "- Wie kann nur nach deutschsprachigen Texten gesucht werden?\n",
    "\n",
    "`````\n",
    "\n",
    "\n",
    "Mit diesem Wissen können wir eine Testabfrage durchführen.\n",
    "\n",
    "Für unsere Abfragen wählen wir JSON als Rückgabeformat aus, weil wir den JSON-String bequem parsen können, indem wir den String mithilfe der Methode .json() in ein Python-Dictionary umwandeln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "# JSON-String in Python Dictionary umwandeln\n",
    "search_results = requests.get(url).json()\n",
    "print(len(search_results[\"items\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Das Dictionary enthält einen Schlüssel \"items\" mit einer Liste der Suchergebnisse als Wert. Die Suchergebnisse sind selbst als Dictionaries organisiert. Jedes Suchergebnis-Dictionary enthält einen Schlüssel \"ocr_eng\" mit den Volltexten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# erstes Suchergebnis auf der ersten Seite der Suchergebnisse\n",
    "search_results[\"items\"][0][\"ocr_eng\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Um die Volltexte für alle Suchergebnisse auf der ersten Seite abzurufen und zu speichern, können wir eine for-Schleife entwerfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Volltexte für die gesamte erste Seite der Suchergebnisse speichern\n",
    "items = search_results[\"items\"]\n",
    "\n",
    "for item in items:\n",
    "    ocr_text = item[\"ocr_eng\"]\n",
    "    title = item[\"title\"]\n",
    "    date = item[\"date\"]\n",
    "    with open(f\"{title}_{date}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(ocr_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Das wollen wir jetzt für alle Suchergebnisse auf allen Seiten reproduzieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Abfrage aller Volltexte mit \"book review\"\n",
    "\n",
    "Da es etwas unübersichtlich ist, wenn die heruntergeladenen Dateien in demselben Ordner liegen wie das Pythonskript, legen wir zunächst in unserem aktuellen Arbeitsverzeichnis (=Ordner, in dem die Jupyter Notebooks liegen) ein neues Verzeichnis an, in dem wir die Volltexte abspeichern werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Neues Verzeichnis anlegen: in diesem Ordner werden die Textdateien gespeichert\n",
    "output_dir = os.path.join(os.getcwd(), \"loc_ocr\")\n",
    "os.makedirs(output_dir, exist_ok=True) # exists_ok: nur erstellen, falls es noch nicht existiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wie gehen wir vor, um jetzt unsere for-Schleife oben nacheinander auf alle Ergebnisseiten anzuwenden? Erinnert euch an die drei Strategien, die wir beim Scrapen der Quotes to Scrape-Website im Abschnitt \"Fortsetzung BeautifulSoup\" diskutiert haben.\n",
    "Eine Idee wäre die Verwendung einer while Schleife mit HTTP Antwort != 200 als break-Bedingung. Diese Strategie ist aber nur anwendbar, wenn beim Abruf einer ungültigen Seite eine HTTP-Antwort ungleich 200 zurückgegeben wird. Das müssen wir zunächst überprüfen: Was passiert, wenn eine nicht existierende Seite aufgerufen wird?\n",
    "Als Beispiel rufen wir die Seite https://chroniclingamerica.loc.gov/search/pages/results/?rows=20&format=json&sequence=0&phrasetext=book+review&andtext=&page=2000 auf.\n",
    "\n",
    "Tatsächlich gibt es eine Umleitung auf Seite 1 mit einem gültigen HTTP-Statuscode! Wir können also in diesem Fall die Strategie mit der while-Schleife nicht verwenden.\n",
    "\n",
    "Eine andere Idee ist die Verwendung einer for-Schleife, die so lange läuft wie es Ergebnisseiten gibt. Dazu müssen wir aber die Gesamtzahl der Ergebnisseiten kennen. Das können wir entweder programmatisch lösen oder manuell aus dem User Interface ablesen. Um die Gesamtzahl der Ergebnisseiten direkt aus Python heraus zu ermitteln, könnten wir zum Beispiel die Anzahl der Ergebnisseiten durch die Anzahl der Ergebnisse auf jeder Ergebnisseite teilen und diese Zahl aufrunden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Gesamtanzahl der Ergebnisseiten ermitteln: Anzahl der Ergebnisse durch Anzahl der Ergebnisse pro Seite teilen\n",
    "base_url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "search_results = requests.get(base_url).json()\n",
    "pages_float = search_results[\"totalItems\"] / search_results[\"itemsPerPage\"]\n",
    "pages = math.ceil(pages_float) # aufrunden\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "`````{admonition} Verständnisfragen\n",
    ":class: tip\n",
    "\n",
    "- Betrachtet die Ergebnisse für unsere Anfrage über das User Interface der Website: https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review. Stimmt die berechnete Anzahl der Ergebnisseiten?\n",
    "- Gibt es noch eine andere Möglichkeit, die letzte Ergebnisseite zu erkennen?\n",
    "- Was bedeuten die Schlüssel `endIndex` und `startIndex`? Welchen Wert haben Sie auf der ersten Ergebnisseite? Welchen Wert auf der letzten?\n",
    "\n",
    "`````\n",
    "\n",
    "Zu der Gesamtzahl der Seiten addieren wir 1, da wir später die range(1, n)-Funktion verwenden wollen, welche eine Integersequenz von Zahl 1 bis Zahl n-1 generiert.\n",
    "\n",
    "Unsere for-Schleife sieht dann so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Volltexte zu allen Ergebnissen von allen Ergebnisseiten speichern\n",
    "for page in range(1, pages + 1):\n",
    "    request_url = f\"{base_url}&page={page}\"\n",
    "    response = requests.get(request_url).json()\n",
    "    # überprüfen, ob die Anfrage erfolgreich war\n",
    "    print(response.status_code)\n",
    "    # for-Schleife für eine einzelne Ergebnisseite einsetzen\n",
    "    items = response[\"items\"]\n",
    "    for item in items:\n",
    "        ocr_text = item[\"ocr_eng\"]\n",
    "        title = item[\"title\"]\n",
    "        date = item[\"date\"]\n",
    "        filename = f\"{title}_{date}.txt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(ocr_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aber Achtung! Beim Ausführen des Codes oben gibt es nach einigen Schleifendurchläufen eine Fehlermeldung: JSONDecodeError: Expecting value: line 1 column 1 (char 0). Die Fehlermeldung entsteht dann, wenn die HTTP-Anfrage keine erfolgreiche Antwort liefert. Dieser Fall kann registriert werden, indem nach jeder Anfrage der Statuscode ausgegeben wird. Warum hat die Anfrage plötzlich keine erfolgreiche Antwort geliefert? Das liegt daran, dass wir uns nicht an die Einschränkungen der LOC gehalten haben und die HTTP-Anfrage dadurch ab einem bestimmten Punkt abgelehnt wird. Wenn wir dann versuchen, den Antwortbody mithilfe der .json()-Methode in ein Python Dictionary umzuwandeln, teilt der Python interpreter uns mit, dass das nicht möglich ist, weil wir die Methode nicht auf einen gültigen JSON-String angewendet haben.\n",
    "\n",
    "Bei der Abfrage von sehr vielen Seiten müssen wir uns also nach den Einschränkungen der LOC richten und bestimmte Abfrageraten (Rate Limits) einhalten.\n",
    "\n",
    "### Rate Limits berücksichtigen und die Abfragerate steuern\n",
    "\n",
    "Wie wir bereits in der kurzen Einführung zu APIs besprochen haben, setzen Webseitenbetreiber:innen für gewöhnlich Grenzen für die Datenabfrage über ihre APIs fest. Manche kommunizieren diese Einschränkungen nur schriftlich in Form der Dokumentation, andere setzen sie technisch fest, sodass wiederholte Anfragen desselben Clients automatisch bei Überschreiten der erlaubten Abfragerate blockiert werden. Um dies zu verhindern und um den Server nicht mit vielen Anfragen, die schnell nacheinander gestellt werden, zu überlasten, muss der Code so geschrieben werden, dass die erlaubte Abfragerate der API eingehalten werden. Dazu können verschiedene Strategien angewandt werden, die in diesem Abschnitt vorgestellt werden.\n",
    "\n",
    "Eine Recherche auf den Seiten der Library of Congress liefert die Seite https://www.loc.gov/apis/json-and-yaml/working-within-limits. Hier legt die LOC Einschränkungen für die der Chronicling America API übergeordnete Seite loc.gov fest. Die Chronicling America API wird zwar nicht explizit erwähnt, aber wir können vermuten, dass die Einschränkungen auch für die Chronicling America API gelten. Da etwas unklar ist, welches Limit für diese API gilt, richten wir uns nach der restriktivsten Vorgabe, nach der nur 20 Abfragen alle 10 Sekunden erlaubt sind. So sind wir in jedem Fall auf der sicheren Seite.\n",
    "\n",
    "Wie können wir also die HTTP-Abfragen auf 20 Abfragen je 10 Sekunden einschränken? Dazu müssen wir den Code so umschreiben, dass innerhalb einer bestimmten Zeit nur eine bestimmte Anzahl an Anfragen gestellt werden.\n",
    "\n",
    "Um die Abfragerate einzuschränken, gibt es mehrere Möglichkeiten:\n",
    "\n",
    "1) **Funktion `time.sleep()` aus dem Paket time**. Die Funktion time.sleep(x) kann in den Schleifenkörper einer for-Schleife eingefügt werden, um den nächsten Schleifendurchlauf um x Sekunden zu verzögern. Diese Methode ist einstiegsfreundlich, aber ungenau, weil die Laufzeit der Schleife selbst nicht in die Wartezeit mit einbezogen wird, sodass der nächste Schleifendurchlauf länger als notwendig verzögert wird."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rate limiting mit time.sleep(): Allgemeines Schema\n",
    "\n",
    "for i in range(1, 6):\n",
    "    print(i)\n",
    "    time.sleep(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Rate limiting mit time.sleep():  LOC API\n",
    "\n",
    "base_url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "search_results = requests.get(base_url).json()\n",
    "pages_float = search_results[\"totalItems\"] / search_results[\"itemsPerPage\"]\n",
    "pages = math.ceil(pages_float) # aufrunden\n",
    "\n",
    "for page in range(1, pages + 1):\n",
    "    request_url = f\"{base_url}&page={page}\"\n",
    "    response = requests.get(request_url).json()\n",
    "    # for-Schleife für eine einzelne Ergebnisseite einsetzen\n",
    "    items = response[\"items\"]\n",
    "    for item in items:\n",
    "        ocr_text = item[\"ocr_eng\"]\n",
    "        title = item[\"title\"]\n",
    "        date = item[\"date\"]\n",
    "        filename = f\"{title}_{date}.txt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(ocr_text)\n",
    "\n",
    "    time.sleep(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) **Python Dekoratoren aus dem Paket ratelimit**. Wesentlich effizienter und eleganter ist die Verwendung von sogenannten Python Dekoratoren bzw. Decorators. Das Paket ratelimit bietet zwei solche Dekoratoren, die dazu verwendet werden können, um zu registrieren, wie häufig eine Funktion nacheinander aufgerufen wird, und die ab einer bestimmten Anzahl wiederholter Aufrufe eine Wartepause erzwingen. Um Decorators verwenden zu können, müssen wir unsere Abfrage jedoch in eine Funktion verpacken. Das Paket ratelimit wie auch die gängigen und immer noch viel verwendeten Alternativen (z.B. ratelimiter), werden allerdings seit einigen Jahren nicht mehr maintained. Das heißt, dass der Code bereits sehr alt ist und Probleme, auf die User:innen die Entwickler:innen des Pakets aufmerksam machen, nicht mehr behoben werden. So hat zum Beispiel GitHub User:in Justin VanWinkle [darauf hingewiesen](https://gist.github.com/justinvanwinkle/d9f04950083c4554835c1a35f9d22dad), dass ratelimit in bestimmten Umständen die Abfragerate nicht zuverlässig kontrolliert."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{note}\n",
    "Python Dekoratoren (Decorators)\n",
    "\n",
    "> A decorator in Python is a function that accepts another function as an argument. The decorator will usually modify or enhance the function it accepted and return the modified function. This means that when you call a decorated function, you will get a function that may be a little different that may have additional features compared with the base definition.\n",
    "\n",
    "Quelle: [Michael Droscill (2017).](https://python101.pythonlibrary.org/chapter25_decorators.html)\n",
    "\n",
    "Dekoratoren beruhen auf einem komplexen Konzept und wir können hier nicht tiefer einsteigen, aber wenn die ein oder andere Person doch etwas tiefer einsteigen will, kann ich diese beiden Ressourcen empfehlen:\n",
    "- Primer on Python Decorators, https://realpython.com/primer-on-python-decorators/\n",
    "- Python Decorators in 15 Minutes, https://www.youtube.com/watch?v=r7Dtus7N4pI\n",
    "\n",
    "Bei der Verwendung der Dekoratoren aus dem Paket ratelimit verwenden wir diese Anleitung von Akshay Ranganath:\n",
    "- Rate Limiting with Python, https://akshayranganath.github.io/Rate-Limiting-With-Python/\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rate limiting mit Python decorators: Allgemeines Schema\n",
    "\n",
    "PERIOD_SEC = 10\n",
    "CALLS_PER_PERIOD_SEC = 3 # 3 Abfragen in 10 Sekunden\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS_PER_PERIOD_SEC, period=PERIOD_SEC)\n",
    "def test_function(x):\n",
    "    print(x)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    test_function(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Rate limiting mit Python decorators: LOC API\n",
    "\n",
    "PERIOD_SEC = 10\n",
    "CALLS_PER_PERIOD_SEC = 20 # 20 Abfragen in 10 Sekunden\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS_PER_PERIOD_SEC, period=PERIOD_SEC)\n",
    "def get_fulltext(url, output_dir):\n",
    "    response = requests.get(url).json()\n",
    "    # for-Schleife für eine einzelne Ergebnisseite einsetzen\n",
    "    items = response[\"items\"]\n",
    "    for item in items:\n",
    "        ocr_text = item[\"ocr_eng\"]\n",
    "        title = item[\"title\"]\n",
    "        date = item[\"date\"]\n",
    "        filename = f\"{title}_{date}.txt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(ocr_text)\n",
    "\n",
    "\n",
    "base_url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "search_results = requests.get(base_url).json()\n",
    "pages_float = search_results[\"totalItems\"] / search_results[\"itemsPerPage\"]\n",
    "pages = math.ceil(pages_float) # aufrunden\n",
    "\n",
    "for page in range(1, pages + 1):\n",
    "    request_url = f\"{base_url}&page={page}\"\n",
    "    get_fulltext(request_url, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Beachtet, dass diese Funktionsdefinition sich in einem wichtigen Aspekt von der Definition der Funktion scrape_quotes() im Abschnitt \"Fortsetzung BeautifulSoup\" unterscheidet: Beim Aufruf der Funktion get_fulltext() wird nur genau eine Anfrage gestellt und die Funktion wird aus einer for-Schleife heraus aufgerufen. Die Funktion scrape_quotes() dagegen stellt beim Aufruf mehrere Anfragen und die for-Schleife, die über die zu scrapenden URLs iteriert, befindet sich in der Funktionsdefinition selbst. Bei der Verwendung der Dekoratoren zum Rate Limiting muss darauf geachtet werden, dass die Funktion so definiert ist wie die get_fulltext()-Funktion. Die Funktionsaufrufe können nämlich nur dann mithilfe der Funktionsaufrufe verzögert werden, wenn die Funktion auch mehrmals aufgerufen wird!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. **Feingranulares Rate Limiting mit dem Paket limits**. Eine Alternative, die etwas mehr Code und Hintergrundwissen erfordert aber dafür auch viele Anpassungsmöglichkeiten bietet, ist das [Paket limits](https://limits.readthedocs.io/en/latest/index.html). Das Paket ist eigentlich eher zur Implementierung von Rate Limiting in Schnittstellen und Softwaresystemen gedacht. Die Dokumentationsseiten beschreiben deswegen viele Anwendungsfälle und Konfigurationen, die recht komplex und für uns nicht relevant sind. Es kann beispielsweise zwischen verschiedenen Rate Limiting-Strategien ausgewählt werden und es kann festgelegt werden, ob die Anzahl der vergangenen Anfragen im Arbeitsspeicher oder einer externen Datenbank gespeichert werden soll. Für unsere Zwecke reicht immer der Arbeitsspeicher und die anderen Optionen können wir ignorieren. Ein möglicher Einsatz von limits zum Überwachen und Kontrollieren der Abfragerate im Rahmen von API-Abfragen könnte so aussehen:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rate limiting mit dem Paket limits: Allgemeines Schema\n",
    "\n",
    "# Rate limit festlegen, z.B. drei Anfragen pro Sekunde. Hier geht auch z.B. \"40/minute\", \"3/2seconds\" oder \"1 per second\" according to https://limits.readthedocs.io/en/latest/api.html#limits.parse\n",
    "rate_limit = parse(\"3/second\")\n",
    "# Speicherort festlegen: Anzahl der vergangenen Anfragen werden im Arbeitsspeicher gehalten (theoretisch ginge auch eine externe Datenbank)\n",
    "memory_storage = storage.MemoryStorage()\n",
    "# Strategie festlegen und Moving window rate limiter Objekt erstellen\n",
    "moving_window = strategies.MovingWindowRateLimiter(memory_storage)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    while not moving_window.test(rate_limit, \"test_requests\"):\n",
    "        print(f\"Rate limit exceeded for iteration {i}, waiting to retry...\")\n",
    "        time.sleep(1) # alternativ 0.01 oder 0.1\n",
    "    moving_window.hit(rate_limit, \"test_requests\")\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T11:27:13.994717Z",
     "start_time": "2024-12-16T11:27:13.992338Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Rate limiting mit Paket limits: LOC API\n",
    "\n",
    "base_url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "search_results = requests.get(base_url).json()\n",
    "pages_float = search_results[\"totalItems\"] / search_results[\"itemsPerPage\"]\n",
    "pages = math.ceil(pages_float) # aufrunden\n",
    "rate_limit = parse(\"20/10seconds\")\n",
    "memory_storage = storage.MemoryStorage()\n",
    "moving_window = strategies.MovingWindowRateLimiter(memory_storage)\n",
    "\n",
    "for page in range(1, pages + 1):\n",
    "    while not moving_window.test(rate_limit, \"loc_requests\"):\n",
    "        print(f\"Rate limit exceeded for {page}, waiting to retry...\")\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    moving_window.hit(rate_limit, \"loc_requests\")\n",
    "    request_url = f\"{base_url}&page={page}\"\n",
    "    response = requests.get(request_url).json()\n",
    "    # for-Schleife für eine einzelne Ergebnisseite einsetzen\n",
    "    items = response[\"items\"]\n",
    "\n",
    "    for item in items:\n",
    "        ocr_text = item[\"ocr_eng\"]\n",
    "        title = item[\"title\"]\n",
    "        date = item[\"date\"]\n",
    "        filename = f\"{title}_{date}.txt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(ocr_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Methode .test() überprüft, ob das Rate Limit bereits erreicht wurde oder nicht, also ob im angegebenen Zeitfenster (hier 10 Sekunden) bereits die erlaubte Anzahl an Anfragen gestellt wurden (hier 20). Wenn das Limit erreicht wurde, wird False zurückgegeben, ansonsten True. Die Bedingung `not moving_window.test(rate_limit, \"loc_requests\")` wird also genau dann True, wenn das Limit erreicht wurde und 20 Anfragen in 10 Sekunden gestellt wurden. In diesem Fall wird gewartet, bis genug Zeit vergangen ist und wieder Anfragen erlaubt sind. Mit der Methode .hit() wird in jedem Durchlauf der for-Schleife eine Anfrage registriert, damit sich der Zähler für die Anzahl der bisher gestellten Anfragen im Objekt memory_storage erhöht. Mehr dazu könnt ihr in den [Dokumentationsseiten des Pakets limits](https://limits.readthedocs.io/en/stable/index.html) nachlesen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "```{note}\n",
    "Konstanten (Constants)\n",
    "\n",
    "Im Code oben verwenden wir Großbuchstaben, um die Variablen `CALLS_PER_PERIOD_SEC` und `PERIOD_SEC` zu benennen. Diese Schreibweise hat sich in Python für Konstanten etabliert, also für Variablen, deren Wert sich im Programmverlauf nicht ändert.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Quellen\n",
    "\n",
    "```{bibliography}\n",
    "   :list: enumerated\n",
    "   :filter: keywords % \"decorators\" or keywords % \"loc\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
