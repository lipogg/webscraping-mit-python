{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Beispiel 2: Library of Congress API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Die Library of Congress (LOC) bietet eine Reihe sehr gut dokumentierter APIs zur Abfrage von Metadaten, Dateien und Volltexten aus dem Bestand der Bibliothek. Eine davon ist die API der Sammlung US-amerikanischer historischer Zeitungen Chronicling America. Diese API werden wir in dieser Stunde kennenlernen.\n",
    "\n",
    "- Übersicht über die LOC APIs: https://guides.loc.gov/digital-scholarship/accessing-digital-materials#s-lib-ctab-26648178-2\n",
    "- Dokumentation zur Chronicling America API: https://chroniclingamerica.loc.gov/about/api/\n",
    "\n",
    "Zunächst machen wir uns mit der Chronicling America API vertraut. Welche Daten können darüber abgefragt werden?\n",
    "\n",
    "Für unser Beispiel werden wir die Volltexte zu allen Ergebnissen einer Suche nach Schlüsselwörtern in den Volltexten der Zeitungen abfragen und herunterladen (Abschnitt \"Searching the directory and newspaper pages using OpenSearch\"). Die Volltexte sind mithilfe von OCR-Verfahren erstellt, also mithilfe von automatischer Bilderkennung. Unsere Suchabfrage liefert also nur diejenigen Zeitungen, in denen die Suchwörter korrekt erkannt wurden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Vorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# wir müssen zunächst die Anaconda Einstellungen ändern, damit wir das Paket ratelimit installieren können:\n",
    "# https://stackoverflow.com/questions/48493505/packagesnotfounderror-the-following-packages-are-not-available-from-current-cha\n",
    "# import sys\n",
    "# !conda config --append channels conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paket ratelimit installieren\n",
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} ratelimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T10:33:16.414953Z",
     "start_time": "2024-12-16T10:33:16.406022Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pakete importieren\n",
    "import requests\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from limits import strategies, storage, parse\n",
    "# Dieses Modul müsst ihr nicht importieren\n",
    "from utils import skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Exploration der Chronicling America API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wie in der letzten Stunde müssen wir zur Abfrage von Daten wieder eine URI nach den Vorgaben der API Dokumentation zusammensetzen.\n",
    "\n",
    "Suchabfragen können mit einem `?` an die URL https://chroniclingamerica.loc.gov/search/pages/results/ angefügt werden.\n",
    "Es gibt laut [Dokumentationsseite](https://chroniclingamerica.loc.gov/about/api/) drei verschiedene Abfrageparameter:\n",
    "\n",
    "- andtext: the search query\n",
    "- format: 'html' (default), or 'json', or 'atom' (optional)\n",
    "- page: for paging results (optional)\n",
    "\n",
    "Diese Parameter werden wir uns der Reihe nach ansehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Parameter andtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Der andtext Parameter: Volltexte nach Schlagwörtern oder Phrasen durchsuchen\n",
    "# Suche nach Schlagwörtern book AND review\n",
    "url_1 = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=book+review\"\n",
    "url_2 = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=book%20review\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Eine aufmerksame Betrachtung der Ergebnisse der Suche nach einem Suchbegriff über die Suchmaske der Website https://chroniclingamerica.loc.gov zeigt, dass die Suche über die Suchmaske genau dieselben Ergebnisse liefert wie die API-Abfrage. Das ist nicht erstaunlich, denn die URL, die beim Verwenden der Suchmaske generiert wird, ist ganz ähnlich aufgebaut wie die URL, die wir für die API-Abfrage erstellen, mit dem einzigen Unterschied, dass die Suchparameter etwas anders aussehen:\n",
    "\n",
    ":::{figure-md}\n",
    "<img src=\"loc_ca_suche.png\" alt=\"Suchmaske Chronicling America\" class=\"bg-transparent\" width=\"80%\">\n",
    "\n",
    "Einfache Suche über die Suchmaske der Seite Chronicling America.\n",
    ":::\n",
    "\n",
    "Wir können diese URL nutzen, um den andtext-Parameter besser zu verstehen. Wenn wir in der einfachen Suche nach dem Suchbegriff \"book review\" suchen, dann steht in der URL \"book+review\". Wenn wir stattdessen die Erweiterte Suche (Tab Advanced Search) verwenden und nach einer Phrase suchen, dann steht in der URL der Zusatz \"&phrasetext=book+review\":\n",
    "\n",
    ":::{figure-md}\n",
    "<img src=\"loc_ca_erweiterte_suche.png\" alt=\"Erweiterte Suche Chronicling America\" class=\"bg-transparent\" width=\"80%\">\n",
    "\n",
    "Erweiterte Suche über die Suchmaske der Seite Chronicling America.\n",
    ":::\n",
    "\n",
    "Tatsächlich akzeptiert auch die Chronicling America API eine Abfrage-URI mit dem Zusatz &phrasetext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Suche nach Phrase \"book review\"\n",
    "url_3 = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review\"\n",
    "# woher weiß ich das: manuelle Suche über \"advanced search\" und URL untersuchen\n",
    "# https://chroniclingamerica.loc.gov/search/pages/results/?dateFilterType=yearRange&date1=1770&date2=1963&language=&ortext=&andtext=&phrasetext=book+review&proxtext=&proxdistance=5&rows=20&searchType=advanced\n",
    "search_results = requests.get(url_3)\n",
    "# search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Parameter format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Der format-Parameter: Ergebnisse im JSON-Format abfragen\n",
    "# https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\n",
    "# erste Seite der Suchergebnisse: 20 Ergebnisse je Seite\n",
    "url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "search_results = requests.get(url)\n",
    "# search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "```{note}\n",
    "JSON im Chrome Browser ansehen\n",
    "\n",
    "Zur Ansicht der JSON-Datei im Chrome Browser können wir wieder auf die Entwicklertools zurückgreifen. Die Standardansicht ist nämlich sehr schwer lesbar, weil der JSON-String nicht formatiert ist. Um eine formatierte Ansicht zu erhalten, befolgt die folgenden Schritte: Entwicklertools öffnen -> \"Sources\"-Tab auswählen-> Link anklicken\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Parameter page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Der page Parameter: Nur die ausgewählte Ergebnisseite abfragen\n",
    "first_page = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json&page=1\"\n",
    "first_page_results = requests.get(first_page)\n",
    "# first_page_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Per Default werden immer die ersten 20 Suchergebnisse (also die erste Seite der Suchergebnisse) ausgegeben, wenn der page-Parameter weggelassen wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "default_page = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "default_page_results = requests.get(default_page)\n",
    "first_page_results.content == default_page_results.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Mit diesem Wissen können wir eine Testabfrage durchführen.\n",
    "\n",
    "Für unsere Abfragen wählen wir JSON als Rückgabeformat aus, weil wir den JSON-String bequem parsen können, indem wir den String mithilfe der Methode .json() in ein Python-Dictionary umwandeln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "# JSON-String in Python Dictionary umwandeln\n",
    "search_results = requests.get(url).json()\n",
    "print(len(search_results[\"items\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Das Dictionary enthält einen Schlüssel \"items\" mit einer Liste der Suchergebnisse als Wert. Die Suchergebnisse sind selbst als Dictionaries organisiert. Jedes Suchergebnis-Dictionary enthält einen Schlüssel \"ocr_eng\" mit den Volltexten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# erstes Suchergebnis auf der ersten Seite der Suchergebnisse\n",
    "search_results[\"items\"][0][\"ocr_eng\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Um die Volltexte für alle Suchergebnisse auf der ersten Seite abzurufen und zu speichern, können wir eine for-Schleife entwerfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Volltexte für die gesamte erste Seite der Suchergebnisse speichern\n",
    "items = search_results[\"items\"]\n",
    "\n",
    "for item in items:\n",
    "    ocr_text = item[\"ocr_eng\"]\n",
    "    title = item[\"title\"]\n",
    "    date = item[\"date\"]\n",
    "    with open(f\"{title}_{date}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(ocr_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Das wollen wir jetzt für alle Suchergebnisse auf allen Seiten reproduzieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Abfrage aller Volltexte mit \"book review\"\n",
    "\n",
    "Da es etwas unübersichtlich ist, wenn die heruntergeladenen Dateien in demselben Ordner liegen wie das Pythonskript, legen wir zunächst in unserem aktuellen Arbeitsverzeichnis (=Ordner, in dem die Jupyter Notebooks liegen) ein neues Verzeichnis an, in dem wir die Volltexte abspeichern werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Neues Verzeichnis anlegen: in diesem Ordner werden die Textdateien gespeichert\n",
    "output_dir = os.path.join(os.getcwd(), \"loc_ocr\")\n",
    "os.makedirs(output_dir, exist_ok=True) # exists_ok: nur erstellen, falls es noch nicht existiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wie gehen wir vor, um jetzt unsere for-Schleife oben nacheinander auf alle Ergebnisseiten anzuwenden?\n",
    "Eine Idee wäre die Verwendung einer while Schleife mit HTTP Antwort != 200 als break-Bedingung. Diese Strategie ist aber nur anwendbar, wenn beim Abruf einer ungültigen Seite eine HTTP-Antwort ungleich 200 zurückgegeben wird. Das müssen wir zunächst überprüfen: Was passiert, wenn eine nicht existierende Seite aufgerufen wird?\n",
    "Als Beispiel rufen wir die Seite https://chroniclingamerica.loc.gov/search/pages/results/?rows=20&format=json&sequence=0&phrasetext=book+review&andtext=&page=1640 auf.\n",
    "\n",
    "Tatsächlich gibt es eine Umleitung auf Seite 1 mit einem gültigen HTTP-Statuscode. Wir können also in diesem Fall die Strategie mit der while-Schleife nicht verwenden.\n",
    "\n",
    "Eine andere Idee ist die Verwendung einer for-Schleife. Dazu müssen wir aber die Gesamtzahl der Ergebnisseiten kennen. Die Gesamtzahl der Ergebnisseiten können wir aber einfach ermitteln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Gesamtanzahl der Ergebnisseiten ermitteln: Anzahl der Ergebnisse durch Anzahl der Ergebnisse pro Seite teilen\n",
    "base_url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "search_results = requests.get(base_url).json()\n",
    "pages_float = search_results[\"totalItems\"] / search_results[\"itemsPerPage\"]\n",
    "pages = math.ceil(pages_float) # aufrunden\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Zu der Gesamtzahl der Seiten addieren wir 1, da wir später die range(1, n)-Funktion verwenden wollen, welche eine Integersequenz von Zahl 1 bis Zahl n-1 generiert.\n",
    "\n",
    "Unsere for-Schleife sieht dann so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Volltexte zu allen Ergebnissen von allen Ergebnisseiten speichern\n",
    "for page in range(1, pages + 1):\n",
    "    request_url = f\"{base_url}&page={page}\"\n",
    "    response = requests.get(request_url).json()\n",
    "    # überprüfen, ob die Anfrage erfolgreich war\n",
    "    print(response.status_code)\n",
    "    # for-Schleife für eine einzelne Ergebnisseite einsetzen\n",
    "    items = response[\"items\"]\n",
    "    for item in items:\n",
    "        ocr_text = item[\"ocr_eng\"]\n",
    "        title = item[\"title\"]\n",
    "        date = item[\"date\"]\n",
    "        filename = f\"{title}_{date}.txt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(ocr_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Aber Achtung! Beim Ausführen des Codes oben gibt es nach einigen Schleifendurchläufen eine Fehlermeldung: JSONDecodeError: Expecting value: line 1 column 1 (char 0). Die Fehlermeldung entsteht dann, wenn die HTTP-Anfrage keine erfolgreiche Antwort liefert. Das liegt mit großer Wahrscheinlichkeit daran, dass wir uns nicht an die Einschränkungen der LOC gehalten haben und die HTTP-Anfrage dadurch ab einem bestimmten Punkt abgelehnt wird. Wenn wir dann versuchen, den Antwortbody mithilfe der .json()-Methode in ein Python Dictionary umzuwandeln, teilt der Python interpreter uns mit, dass das nicht möglich ist, weil wir die Methode nicht auf einen gültigen JSON-String angewendet haben.\n",
    "\n",
    "Bei der Abfrage von sehr vielen Seiten müssen wir uns also nach den Einschränkungen der LOC richten und bestimmte Abfrageraten (Rate Limits) einhalten. \n",
    "\n",
    "### Rate Limiting: Die Abfragerate steuern\n",
    "\n",
    "Die LOC hat Einschränkungen für die der Chronicling America API übergeordnete Seite loc.gov festgelegt, und wir können vermuten, dass die Einschränkungen auch für die Chronicling America API gelten:  https://www.loc.gov/apis/json-and-yaml/working-within-limits\n",
    "\n",
    "Um auf der sicheren Seite zu sein, richten wir uns nach der restriktivsten Vorgabe, nach der nur 20 Abfragen alle 10 Sekunden erlaubt sind.\n",
    "\n",
    "Wie können wir also die HTTP-Abfragen auf 20 Abfragen je 10 Sekunden einschränken? Dazu müssen wir Strategien zum \"Rate Limiting\" anwenden, das heißt wir müssen den Code so umschreiben, dass innerhalb einer bestimmten Zeit nur eine bestimmte Anzahl an Anfragen gestellt werden.\n",
    "\n",
    "Um die Abfragerate einzuschränken, gibt es mehrere Möglichkeiten:\n",
    "\n",
    "1) **Funktion `time.sleep()` aus dem Paket time**. Die Funktion time.sleep(x) kann in den Schleifenkörper einer for-Schleife eingefügt werden, um den nächsten Schleifendurchlauf um x Sekunden zu verzögern. Diese Methode ist einstiegsfreundlich, aber ungenau, weil die Laufzeit der Schleife selbst nicht in die Wartezeit mit einbezogen wird, sodass der nächste Schleifendurchlauf länger als notwendig verzögert wird.\n",
    "2) **Python Dekoratoren aus dem Paket ratelimit**. Wesentlich effizienter und eleganter ist die Verwendung von sogenannten Python Dekoratoren bzw. Decorators. Das Paket ratelimit bietet zwei solche Dekoratoren, die dazu verwendet werden können, um zu registrieren, wie häufig eine Funktion nacheinander aufgerufen wird, und die ab einer bestimmten Anzahl wiederholter Aufrufe eine Wartepause erzwingen. Um Decorators verwenden zu können, müssen wir unsere Abfrage jedoch in eine Funktion verpacken. Das Paket ratelimit wie auch die gängigen und immer noch viel verwendeten Alternativen (z.B. ratelimiter), werden allerdings seit einigen Jahren nicht mehr maintained. Das heißt, dass der Code bereits sehr alt ist und Probleme, auf die User:innen die Entwickler:innen des Pakets aufmerksam machen, nicht mehr behoben werden. So hat zum Beispiel GitHub User:in Justin VanWinkle [darauf hingewiesen](https://gist.github.com/justinvanwinkle/d9f04950083c4554835c1a35f9d22dad), dass ratelimit in bestimmten Umständen die Abfragerate nicht zuverlässig kontrolliert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "```{note}\n",
    "Python Dekoratoren (Decorators)\n",
    "\n",
    "> A decorator in Python is a function that accepts another function as an argument. The decorator will usually modify or enhance the function it accepted and return the modified function. This means that when you call a decorated function, you will get a function that may be a little different that may have additional features compared with the base definition.\n",
    "\n",
    "Quelle: [Michael Droscill (2017).](https://python101.pythonlibrary.org/chapter25_decorators.html)\n",
    "\n",
    "Dekoratoren beruhen auf einem komplexen Konzept und wir können hier nicht tiefer einsteigen, aber wenn die ein oder andere Person doch etwas tiefer einsteigen will, kann ich diese beiden Ressourcen empfehlen:\n",
    "- Primer on Python Decorators, https://realpython.com/primer-on-python-decorators/\n",
    "- Python Decorators in 15 Minutes, https://www.youtube.com/watch?v=r7Dtus7N4pI\n",
    "\n",
    "Bei der Verwendung der Dekoratoren aus dem Paket ratelimit verwenden wir diese Anleitung von Akshay Ranganath:\n",
    "- Rate Limiting with Python, https://akshayranganath.github.io/Rate-Limiting-With-Python/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3. **Feingranulares Rate Limiting mit dem Paket limits**. Eine Alternative, die etwas mehr Code und Hintergrundwissen erfordert aber dafür auch viele Anpassungsmöglichkeiten bietet, ist das [Paket limits](https://limits.readthedocs.io/en/latest/index.html). Es kann zwischen verschiedenen Rate Limiting-Strategien ausgewählt werden und es kann festgelegt werden, ob die Anzahl der vergangenen Anfragen im Arbeitsspeicher oder einer externen Datenbank gespeichert werden soll (das ist für unsere Zwecke aber nicht notwendig).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# 1) Rate limiting mit time.sleep()\n",
    "\n",
    "base_url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "search_results = requests.get(base_url).json()\n",
    "pages_float = search_results[\"totalItems\"] / search_results[\"itemsPerPage\"]\n",
    "pages = math.ceil(pages_float) # aufrunden\n",
    "\n",
    "for page in range(1, pages + 1):\n",
    "    request_url = f\"{base_url}&page={page}\"\n",
    "    response = requests.get(request_url).json()\n",
    "    # for-Schleife für eine einzelne Ergebnisseite einsetzen\n",
    "    items = response[\"items\"]\n",
    "    for item in items:\n",
    "        ocr_text = item[\"ocr_eng\"]\n",
    "        title = item[\"title\"]\n",
    "        date = item[\"date\"]\n",
    "        filename = f\"{title}_{date}.txt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(ocr_text)\n",
    "\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# 2) Rate limiting mit Python decorators\n",
    "\n",
    "PERIOD_SEC = 10\n",
    "CALLS_PER_PERIOD_SEC = 20 # 20 Abfragen in 10 Sekunden\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS_PER_PERIOD_SEC, period=PERIOD_SEC)\n",
    "def get_fulltext(url):\n",
    "    response = requests.get(url).json()\n",
    "    # for-Schleife für eine einzelne Ergebnisseite einsetzen\n",
    "    items = response[\"items\"]\n",
    "    for item in items:\n",
    "        ocr_text = item[\"ocr_eng\"]\n",
    "        title = item[\"title\"]\n",
    "        date = item[\"date\"]\n",
    "        filename = f\"{title}_{date}.txt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(ocr_text)\n",
    "\n",
    "\n",
    "base_url = \"https://chroniclingamerica.loc.gov/search/pages/results/?andtext=&phrasetext=book+review&format=json\"\n",
    "search_results = requests.get(base_url).json()\n",
    "pages_float = search_results[\"totalItems\"] / search_results[\"itemsPerPage\"]\n",
    "pages = math.ceil(pages_float) # aufrunden\n",
    "\n",
    "for page in range(1, pages + 1):\n",
    "    request_url = f\"{base_url}&page={page}\"\n",
    "    get_fulltext(request_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Beachtet, dass diese Funktionsdefinition sich in einem wichtigen Aspekt von der Definition der Funktion scrape_quotes() im Abschnitt \"Fortsetzung BeautifulSoup\" unterscheidet: Beim Aufruf der Funktion get_fulltext() wird nur genau eine Anfrage gestellt und die Funktion wird aus einer for-Schleife heraus aufgerufen. Die Funktion scrape_quotes() dagegen stellt beim Aufruf mehrere Anfragen und die for-Schleife, die über die zu scrapenden URLs iteriert, befindet sich in der Funktionsdefinition selbst. Bei der Verwendung der Dekoratoren zum Rate Limiting muss darauf geachtet werden, dass die Funktion so definiert ist wie die get_fulltext()-Funktion. Die Funktionsaufrufe können nämlich nur dann mithilfe der Funktionsaufrufe verzögert werden, wenn die Funktion auch mehrmals aufgerufen wird!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T11:27:13.994717Z",
     "start_time": "2024-12-16T11:27:13.992338Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# 3) Rate limiting mit Paket limits\n",
    "\n",
    "# kommt noch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "```{note}\n",
    "Konstanten (Constants)\n",
    "\n",
    "Im Code oben verwenden wir Großbuchstaben, um die beiden Variablen `CALLS_PER_PERIOD_SEC` und `PERIOD_SEC` zu benennen. Diese Schreibweise hat sich in Python für Konstanten etabliert, also für Variablen, deren Wert sich im Programmverlauf nicht ändert.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Quellen\n",
    "\n",
    "```{bibliography}\n",
    "   :list: enumerated\n",
    "   :filter: keywords % \"decorators\" or keywords % \"loc\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
